{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: lcms_data_processing\n",
    "date: 09/25/2024\n",
    "version: 1.1\n",
    "author: Justin Sankey, Johanna Ganglbauer\n",
    "\n",
    "description: Takes raw liquit chromatography mass spectroscopy (LCMS) data, computes recovery rates, MDLs, and ratios of default channel to MS TOF channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the notebook for the first time you need to install all required python packages.\n",
    "So, type the following commands in your python console or anaconda prompt:\n",
    "- pip install pandas\n",
    "- pip install numpy\n",
    "- pip install matplotlib\n",
    "- pip install seaborn\n",
    "- pip install scikit-learn\n",
    "- pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "import math as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only cell that you should have to make edits to. \\\n",
    "(1) Enter in your desired input and output file paths. The input file is the exported table from the Sciex Analyst Software, with....(add description or SCREENSHOTS from SCIEX Analyst Software)\n",
    "(2) Change what you deem to be an acceptable recovery range.\n",
    "(3) Indicate which samples you want to use to calculate MDL values. You can only use terms which are used in the \"Sample Comment\" column of your input data.\n",
    "(4) Introduce the hard facts about your sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data upload file path\n",
    "# raw_filepath = r'example_data_raw\\20240903_pfas_kynol_ks_single_compound.csv'\n",
    "# raw_filepath = r'example_data_raw\\20230703_ACF_Batch1_update_20240923.txt'\n",
    "raw_filepath = r'example_data_raw\\test_data.txt'\n",
    "\n",
    "# processed data output excel file path\n",
    "processed_filepath =r'example_data_processed\\test.xlsx'\n",
    "# processed_filepath =r'example_data_processed\\PFAS_ACF_Batch1.xlsx'\n",
    "\n",
    "# processed data long format\n",
    "processed_filepath_long =r'example_data_processed\\test.csv'\n",
    "# processed_filepath_long =r'example_data_processed\\PFAS_ACF_Batch1.csv'\n",
    "\n",
    "# directory to save plots to\n",
    "plot_directory = r'example_figures'\n",
    "  \n",
    "# file paths for IDL and IQL data - not meant to be adopted\n",
    "IDL_2024_filepath = r'example_data_raw\\IDL_2024.csv'\n",
    "IDL_IQL_filepath = r'example_data_raw\\IDL_IQL.csv'\n",
    "\n",
    "#color-coding for recoveries table\n",
    "in_range = 'background-color: green'\n",
    "in_range_min_val = 0.6 \n",
    "in_range_max_val = 1.4\n",
    "out_range = 'background-color: red'\n",
    "out_range_min_val = 0.4 \n",
    "out_range_max_val = 1.8\n",
    "question_range = 'background-color: yellow'\n",
    "\n",
    "# threshold for acceptance of absolute percentage difference between default channel and TOF MS channel\n",
    "allowed_channel_deviation = 30\n",
    "\n",
    "# samples used for MDL calculatation - keywords need to be used in \"Sample Comment\" column of input data\n",
    "# if you want to use IDLs only, use an empty list.\n",
    "mdl_selection = [\n",
    "    'IS Check', 'Process Blank', 'Water Extraction Blank'\n",
    "]\n",
    "\n",
    "# hard facts about your sample - to correct wrong IPS concentrations and convert the PFAS concentration.\n",
    "ips_concentration_calibration = 4  # ng/ml\n",
    "extracted_sample_volume = 0.5  # ml\n",
    "sample_unit = 'l'  # eiter 'l' for liter or 'g' for gram\n",
    "nonextracted_sample_quantity = 1 # indicate the weight (if sample_unit is 'g') or volume (if sample unit is 'l') or your non-extracted sample.\n",
    "\n",
    "correct_ips_concentration = True # set True if you want to correct the IPS concentration in your samples by the average IPS area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse inputs\n",
    "if sample_unit not in ['l', 'g']:\n",
    "    raise Exception(\"\"\"Please use either 'g' or 'l' for variable sample_unit. \"\"\")\n",
    "\n",
    "if not type(correct_ips_concentration) == bool:\n",
    "    raise Exception(\"\"\"Please use either True or False for for variable correct_tables. \"\"\")\n",
    "\n",
    "# Ensure file path and folder path exist to write outputs to and create folders, if they do not exist\n",
    "folder_path = os.path.dirname(processed_filepath)\n",
    "if folder_path and not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path) \n",
    "    \n",
    "folder_path = os.path.dirname(processed_filepath_long)\n",
    "if folder_path and not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path) \n",
    "    \n",
    "if not os.path.exists(plot_directory):\n",
    "    os.makedirs(plot_directory)\n",
    "\n",
    "# Load input data file\n",
    "if raw_filepath[-4:] == '.csv':\n",
    "    data = pd.read_csv(raw_filepath, delimiter=',', encoding='utf-8', low_memory=False, header=0,)\n",
    "elif raw_filepath[-4:] == '.txt':\n",
    "    data = pd.read_csv(raw_filepath, delimiter='\\t', encoding='utf-8', low_memory=False, header=0,)\n",
    "\n",
    "# Clean up messy 'calculated concentration' column - set all strange strings to NaN\n",
    "data['Calculated Concentration'] = data['Calculated Concentration'].replace(\n",
    "    {'<1 points': np.nan, '< 0': np.nan, 'no root': np.nan, 'NaN': np.nan}\n",
    "    ).astype('float')\n",
    "\n",
    "# Replace 'IPS-13C2_PFOA' values: optional, indicate your input filename if this step is needed.\n",
    "if raw_filepath == r'example_data_raw\\20240903_pfas_kynol_ks_single_compound.csv':\n",
    "    data['Component Group Name'] = data['Component Group Name'].replace('IPS-13C2_PFOA', 'IPS-13C4_PFOA')\n",
    "\n",
    "    # Find rows where 'Component Group Name' is 'IPS-13C4_PFOA' (after replacement)\n",
    "    mask = data['Component Group Name'] == 'IPS-13C4_PFOA'\n",
    "\n",
    "    # Iterate through each of these rows and replace area in column\n",
    "    for idx, row in data[mask].iterrows():\n",
    "        sample_name = row['Sample Name']\n",
    "        \n",
    "        # Find the corresponding row with 'Component Name' == 'IPS-13C4_PFOA' and the same 'Sample Name'\n",
    "        matching_row = data[(data['Component Name'] == 'IPS-13C4_PFOA') & (data['Sample Name'] == sample_name)]\n",
    "        \n",
    "        if not matching_row.empty:\n",
    "            # Update the 'Area IPS' with the value from 'Area' in the matching row\n",
    "            data.at[idx, 'Area IPS'] = matching_row['Area'].values[0]\n",
    "\n",
    "# Correct channel names in original data (all of the TOF channels are labelled by _TOF MS, only 2 of them are labeled by only _TOF)\n",
    "mask_names = data['Component Name'].str.endswith('_TOF')\n",
    "data['Component Name'][mask_names] = [compound + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n",
    "\n",
    "# Split data into quantification data, calibration data, and blanks for mdl calculation\n",
    "calibration_only = data[(data['Sample Type'] == 'Standard')]\n",
    "quantification_blank = data[(data['Sample Type'] != 'Standard')]\n",
    "\n",
    "if not quantification_blank.empty:\n",
    "    # select data for computation of mdl and exclude it from quantification data\n",
    "    if mdl_selection == []:\n",
    "        blank_only = None\n",
    "        quantification_only = quantification_blank\n",
    "    else:\n",
    "        blank_selection = quantification_blank['Sample Comment'].str.contains('|'.join(mdl_selection))\n",
    "        if sum(blank_selection) == 0:\n",
    "            (f'Be careful, no samples have been collected for the MDL calculation because {mdl_selection} is not a Sample Comment.')\n",
    "            print(f'If you do not select blank set variable mdl_selection to [].')\n",
    "        if sum(blank_selection) == len(quantification_blank):\n",
    "            print(f'Be careful, all samples have been collected for the MDL calculation.')\n",
    "            print(f'If this is not what you want reset the variable mdl_selection in the top block.')\n",
    "        blank_selection.replace({np.nan: False}, inplace=True)\n",
    "        blank_only = quantification_blank[blank_selection]\n",
    "        quantification_only = quantification_blank[~blank_selection]\n",
    "\n",
    "# display settings \n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full width of columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid misunderstandings, in the following two abbreviations are extensively used:\n",
    "- IDA: isotope dilution analysis, also known as SS=surrogate standard or EIS=extracted internal standard\n",
    "- IPS: isotope performance standard, also known as IS=injection standard or NIS=non-extracted internal standard\n",
    "\n",
    "The following two blocks calculate response factors from calibration data: ratio of (i) calculated area of IDA * **actual concentration of IPS** and (ii) calculated area of IPS * actual concentration of IDA. \\\n",
    "The data is saved to an excel file and a boxplot of response factors is created.\n",
    "\n",
    "Note: The response factor calculation within sciex uses the ratio of (i) calculated area of IDA and (ii) calculated area of IPS * actual concentration of IDA. \\\n",
    "As the concentration of IPS is missing in the calculation, the response factors deviate by a factor of 4, which is the actual concentration of IPS in the calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define funtion which calculates the IDA IPS ratio.\n",
    "# challenge - search right IPS row indicated in the Component Group Name of IDA.\n",
    "def calulate_ida_ips_ratio(data: pd.DataFrame, column_name:str) -> pd.DataFrame:    \n",
    "    \"\"\"Calculates IDA area times IPS concentration divided by IPS area times IDA concentration and save the results in the indicated column.\n",
    "\n",
    "    :param data: Entire data junk (including all rows and the following columns:\n",
    "    Component Name, Sample Index, Component Group Name, Actual Concentration, Area, and IDA Average Response Factor\n",
    "    :type data: pd.DataFrame\n",
    "    :param column_name: name of column, the calculated ratio should be saved to\n",
    "    :type column_name: str\n",
    "    :return: Data junk only containing IDA rows with the corresponding ratio saved to new column\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\" \n",
    "    data_only_ida = data[data['Component Name'].str.contains('IDA')] \n",
    "    data_only_ida.loc[:,f'{column_name}'] = [np.nan] * len(data_only_ida)\n",
    "    data_only_ida.loc[:,'IPS Area'] = [np.nan] * len(data_only_ida)\n",
    "    data_only_ida.loc[:,'IPS Concentration'] = [np.nan] * len(data_only_ida)\n",
    "    for row_index in data_only_ida.index:\n",
    "        sample_index = data_only_ida.loc[row_index, 'Sample Index']\n",
    "        ips_channel_name = data_only_ida.loc[row_index, 'Component Group Name']\n",
    "        corresponding_ips_area_row = data[(\n",
    "            (data['Sample Index'] == sample_index) &\n",
    "            (data['Component Name'] == ips_channel_name)\n",
    "            )]\n",
    "        data_only_ida.loc[row_index, 'IPS Area'] = corresponding_ips_area_row['Area'].iloc[0]\n",
    "        data_only_ida.loc[row_index, 'IPS Concentration'] = corresponding_ips_area_row['Actual Concentration'].iloc[0]\n",
    "        data_only_ida.loc[row_index, f'{column_name}'] = \\\n",
    "            (data_only_ida.loc[row_index, 'Area'] * corresponding_ips_area_row['Actual Concentration']).iloc[0] \\\n",
    "            / (corresponding_ips_area_row['Area'] * data_only_ida.loc[row_index, 'Actual Concentration']).iloc[0]\n",
    "    \n",
    "    return data_only_ida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values of IDAs and IPS\n",
    "# Save basic sample information as well as areas of intensity peak, actual concentration and Component Group Name.\n",
    "# The 'Component Group Name' is useful to assoiciate the right IPS to each IDA.\n",
    "selected_columns = ['Sample Name', 'Sample Index','Component Name', 'Component Group Name', 'Area', 'Actual Concentration', 'IDA Average Response Factor']\n",
    "calibration_only_ida = calibration_only[selected_columns]\n",
    "\n",
    "# calucluate IDA IPS ratio to compute response factors with calibration data\n",
    "calibration_only_ida = calulate_ida_ips_ratio(\n",
    "    data=calibration_only_ida, column_name='Response Factor from Scratch',\n",
    "    )\n",
    "\n",
    "# create data frame with this response factor calculation (from scratch), the standard deviation and the original values evaluated by Sciex,\n",
    "response_factor = calibration_only_ida.groupby('Component Name', as_index=False)['Response Factor from Scratch'].mean()\n",
    "response_factor['Response Factor Std'] = calibration_only_ida.groupby('Component Name')['Response Factor from Scratch'].std().to_list()\n",
    "response_factor['Response Factor Sciex'] = calibration_only_ida.groupby('Component Name')['IDA Average Response Factor'].mean().to_list()\n",
    "response_factor.rename(columns={'Response Factor from Scratch': 'Response Factor Mean'}, inplace=True)\n",
    "response_factor.index = response_factor['Component Name']\n",
    "response_factor.drop(columns=['Component Name'], inplace=True)\n",
    "\n",
    "# write response factor to excel file\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl') as writer:\n",
    "    response_factor.to_excel(writer, sheet_name='Response Factor')\n",
    "\n",
    "# create and save response factor box plots\n",
    "image_path = os.path.join(plot_directory, 'response_factors.png')\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "calibration_only_ida.boxplot(column='Response Factor from Scratch', by='Component Name', ax=ax, label='data')\n",
    "ax.plot([np.nan] + (response_factor['Response Factor Sciex'] * 4).to_list(), color='red', linestyle='', marker=\"o\", label='4 * RF Sciex')\n",
    "ax.plot([np.nan] + (response_factor['Response Factor Sciex']).to_list(), color='blue', linestyle='', marker=\"o\", label='RF Sciex')\n",
    "fig.suptitle('')\n",
    "ax.set_title('')\n",
    "plt.ylabel('Response Factor (IDA area/IPS area)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.savefig(image_path)\n",
    "plt.show()\n",
    "\n",
    "# save response factor box plot to excel file\n",
    "workbook = load_workbook(processed_filepath)\n",
    "plot_sheet = workbook.create_sheet('Details RF')\n",
    "\n",
    "img = Image(os.path.join(plot_directory, 'response_factors.png'))\n",
    "\n",
    "cell_position = plot_sheet.cell(row=1, column=1).coordinate\n",
    "plot_sheet.add_image(img, cell_position)\n",
    "\n",
    "workbook.save(processed_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is useful to detect and correct wrong information about the amount of IPS added to the samples.\n",
    "The ratio of mean IPS area in the samples and mean IPS area in the calibration data is a good indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IPS average area in calibration and the remaining data (quantification_blank)\n",
    "calibration_ips_area_average = calibration_only.loc[calibration_only['Component Name'].str.contains('IPS'), 'Area'].mean()\n",
    "quantification_blank_ips_area_average = quantification_blank.loc[quantification_blank['Component Name'].str.contains('IPS'), 'Area'].mean()\n",
    "\n",
    "# get IPS and IDA average concentration in the remaining data (quantification_blank).\n",
    "quantification_blank_ips_concentration_average = quantification_blank.loc[quantification_blank['Component Name'].str.contains('IPS'), 'Actual Concentration'].mean()\n",
    "quantification_blank_ida_concentration_average = quantification_blank.loc[quantification_blank['Component Name'].str.contains('IDA'), 'Actual Concentration'].mean()\n",
    "\n",
    "# calculate the \"predicted\" amount of IPS injected to the samples based on the average IPS areas in the quantification_blank data compared to the calibration data.\n",
    "predicted_ips_injection = quantification_blank_ips_area_average * extracted_sample_volume * ips_concentration_calibration / calibration_ips_area_average\n",
    "\n",
    "# Print preliminary results\n",
    "print(f\"It looks like you added {predicted_ips_injection} ng IPS to your samples.\")\n",
    "print(f'You indicated on average {quantification_blank_ips_concentration_average} ng IPS in your samples')\n",
    "print(f'You indicated on average {quantification_blank_ida_concentration_average} ng IDA in your samples')\n",
    "\n",
    "# if correct_tables is set True, the 'Actual Concentration' Values for all IPS components in the quantification data will be corrected.\n",
    "if correct_ips_concentration:\n",
    "    ips_concentration = np.round(predicted_ips_injection, 1)\n",
    "    quantification_only.loc[quantification_only['Component Name'].str.contains('IPS'), 'Actual Concentration'] = ips_concentration\n",
    "    print(f'The column Actual Concentration of the IPS compounds has been corrected to {ips_concentration} ng. Please make sure the indicated IDA volume is right.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block calculates recovery rates for each IDA compound in each sample.\n",
    "$$\n",
    "recovery~rate = \\frac{\\frac{area_{IDA~sample}~\\cdot~concentration_{IPS~sample}}{area_{IPS~sample}~\\cdot~concentration_{IDA~sample}}}{average(\\frac{area_{IDA~calibration}~\\cdot~concentration_{IPS~calibration}}{(area_{IPS~calibration}~\\cdot~concentration_{IDA~calibration})})} = \\frac{ratio}{response~factor}\n",
    "$$\n",
    "\n",
    "Followingly, the right recovery rate is assigned to each PFAS. Each PFAS component has a corresponding IDA - the recovery rate is deduced from the recovery rate of the corresponding IDA.\n",
    "The assignment works for each sample by using the 'IS Name' column which provides information on which IDA standard is associated to which PFAS.\n",
    "\n",
    "The uncertainty of the recovery rate is indicate by a maximum error method - for now, you can just ignore this part...\n",
    "$$\n",
    "\\Delta recovery~rate = \\Delta response~factor * \\frac{ratio}{response~factor^{2}} + \\Delta ratio * \\frac{1}{response~factor}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ida rows from quantification data and calculate ida ips ratio\n",
    "selected_columns = [\n",
    "    'Sample Name', 'Sample Index', 'Component Name', 'Component Group Name', 'Area', 'Actual Concentration', 'Reported Recovery',\n",
    "    ]\n",
    "quantification_ida = quantification_only[selected_columns]\n",
    "quantification_ida = calulate_ida_ips_ratio(\n",
    "    data=quantification_ida, column_name=\"IDA-IPS Ratio\",\n",
    "    )\n",
    "\n",
    "# Assign right response factor to each IDA\n",
    "quantification_ida.loc[:, 'Response Factor Mean'] = [np.nan] * len(quantification_ida)\n",
    "quantification_ida.loc[:, 'Response Factor Std'] = [np.nan] * len(quantification_ida)\n",
    "\n",
    "for component in response_factor.index:\n",
    "    quantification_ida.loc[quantification_ida['Component Name'] == component, 'Response Factor Mean'] = response_factor.loc[component, 'Response Factor Mean']\n",
    "    quantification_ida.loc[quantification_ida['Component Name'] == component, 'Response Factor Std'] = response_factor.loc[component, 'Response Factor Std']\n",
    "\n",
    "# Calculate recovery rate and its related uncertainty\n",
    "quantification_ida.loc[:, 'Recovery Rate'] = quantification_ida.loc[:, 'IDA-IPS Ratio'] / quantification_ida.loc[:, 'Response Factor Mean']\n",
    "quantification_ida.loc[:, 'Recovery Rate Uncertainty 1'] = quantification_ida.loc[:,'Response Factor Std'] * quantification_ida.loc[:,'IDA-IPS Ratio'] \\\n",
    "     / (quantification_ida.loc[:, 'Response Factor Mean'] * quantification_ida.loc[:, 'Response Factor Mean'])\n",
    "\n",
    "quantification_ida[\"Recovery Rate\"] = 100 * quantification_ida[\"Recovery Rate\"] \n",
    "\n",
    "# Plot recovery rates vs sciex recovery rates\n",
    "for title, group in quantification_ida.groupby('Component Name'):\n",
    "    plt.figure()\n",
    "    group.plot(x='Recovery Rate', y='Reported Recovery', title=title, kind='scatter', xlim=[0, 200], ylim=[0, 400], grid=True)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize data frame for following assignments, select only needed columns and use reasonable naming.\n",
    "selected_columns = [\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time', 'Component Name', 'IS Name', 'Calculated Concentration', 'Area',\n",
    "    ]\n",
    "quantification_pfas_default = quantification_only[selected_columns]\n",
    "quantification_pfas_default.rename(columns={'IS Name': 'IDA Name'}, inplace=True)\n",
    "\n",
    "# Get only PFAS default channels\n",
    "quantification_pfas_default = quantification_pfas_default[~quantification_pfas_default['Component Name'].str.contains('IDA|IPS|13C|MS')]\n",
    "\n",
    "# Assign right recovery rate and uncertainty to each PFAS compound\n",
    "quantification_pfas_default.loc[:,'IDA Area'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'IDA Concentration'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'IPS Area'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'IPS Concentration'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'IPS Name'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'Recovery Rate'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'Recovery Rate Uncertainty 1'] = [np.nan] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:,'Recovery Rate Sciex'] = [np.nan] * len(quantification_pfas_default)\n",
    "for row_index in quantification_pfas_default.index:\n",
    "    sample_index = quantification_pfas_default.loc[row_index, 'Sample Index']\n",
    "    ida_channel_name = quantification_pfas_default.loc[row_index, 'IDA Name']\n",
    "    recovery_row = quantification_ida[(\n",
    "        (quantification_ida['Component Name'] == ida_channel_name) &\n",
    "        (quantification_ida['Sample Index'] == sample_index)\n",
    "    )]\n",
    "    quantification_pfas_default.loc[row_index,'IDA Area'] = recovery_row.loc[:,'Area'].to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index,'IDA Concentration'] = recovery_row.loc[:,'Actual Concentration'].to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index,'IPS Area'] = recovery_row.loc[:,'IPS Area'].to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index,'IPS Concentration'] = recovery_row.loc[:,'IPS Concentration'].to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index, 'IPS Name'] = \\\n",
    "        recovery_row.loc[:,'Component Group Name'].to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index, 'Recovery Rate'] = \\\n",
    "        (recovery_row.loc[:,'Recovery Rate']).to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index, 'Recovery Rate Uncertainty 1'] = \\\n",
    "        (recovery_row.loc[:,'Recovery Rate Uncertainty 1'].round(decimals=3) * 100).to_list()[0]\n",
    "    quantification_pfas_default.loc[row_index, 'Recovery Rate Sciex'] = \\\n",
    "        recovery_row.loc[:,'Reported Recovery'].to_list()[0]\n",
    "    \n",
    "    for column in ['Recovery Rate', 'Recovery Rate Uncertainty 1', 'Recovery Rate Sciex']:\n",
    "        quantification_pfas_default[f'{column}'] = quantification_pfas_default[f'{column}'].round(decimals=1)\n",
    "\n",
    "# Put recovery rate in pivot table\n",
    "recovery = quantification_pfas_default.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values='Recovery Rate', aggfunc='first', dropna=False,\n",
    "    )\n",
    "\n",
    "# Write to excel file\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl', mode='a') as writer:\n",
    "    recovery.to_excel(writer, sheet_name='Recovery Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of sample ida - ips ratio and calibration ida - ips ratio (refered to as response factor above) - Method Justin:\n",
    "Uses columns \"IDA area\" and \"IPS area\" directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area IDA values for IDA components\n",
    "selected_columns_area = ['Sample Name', 'Sample Index','Acquisition Date & Time','Component Name', 'Area IDA']\n",
    "area_ida = quantification_blank[selected_columns_area]\n",
    "area_ida = area_ida[area_ida['Component Name'].str.contains('IDA')]\n",
    "area_ida.loc[:,'Sample Name Date'] = area_ida['Sample Name'].astype(str) + \"_\" + area_ida['Acquisition Date & Time']\n",
    "\n",
    "# Create pivot table with Sample name as the index, component name as the column headers, and area as the values\n",
    "area_ida_piv = area_ida.pivot_table(\n",
    "    index=('Sample Name Date',), columns='Component Name', values='Area IDA', aggfunc='first', dropna=False,\n",
    "    )\n",
    "area_ida_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration Area IDA Average for IDA components\n",
    "selected_columns_area = ['Sample Name', 'Sample Index','Acquisition Date & Time','Component Name', 'Area IDA']\n",
    "cal_area_ida = calibration_only[selected_columns_area]\n",
    "cal_area_ida = cal_area_ida[cal_area_ida['Component Name'].str.contains('IDA')]\n",
    "cal_area_ida.loc[:,'Sample Name Date'] = cal_area_ida['Sample Name'].astype(str) + \"_\" + cal_area_ida['Acquisition Date & Time']\n",
    "\n",
    "# Create pivot table with Sample name as the index, component name as the column headers, and area as the values\n",
    "cal_area_ida_piv = cal_area_ida.pivot_table(\n",
    "    index=('Sample Name Date',), columns='Component Name', values='Area IDA', aggfunc='first', dropna=False,\n",
    "    )\n",
    "#Establishes a row with average of each column\n",
    "mean_cal = cal_area_ida_piv.mean(skipna=True)\n",
    "mean_cal=pd.DataFrame(mean_cal).T\n",
    "mean_cal.index=['Average']\n",
    "cal_area_ida_piv = pd.concat([cal_area_ida_piv,mean_cal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPS Area Values for IDA components\n",
    "selected_columns_area = ['Sample Name', 'Sample Index','Acquisition Date & Time','Component Name', 'Area IPS']\n",
    "area_ips = quantification_blank[selected_columns_area]\n",
    "area_ips = area_ips[area_ips['Component Name'].str.contains('IDA')]\n",
    "area_ips.loc[:,'Sample Name Date'] = area_ips['Sample Name'].astype(str) + \"_\" + area_ips['Acquisition Date & Time']\n",
    "\n",
    "# Create pivot table with Sample name as the index, component name as the column headers, and area as the values\n",
    "area_ips_piv = area_ips.pivot_table(\n",
    "    index=('Sample Name Date',), columns='Component Name', values='Area IPS', aggfunc='first', dropna=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPS Calibration Area and Average for IDA components\n",
    "selected_columns_area = ['Sample Name', 'Sample Index','Acquisition Date & Time','Component Name', 'Area IPS']\n",
    "cal_area_ips = calibration_only[selected_columns_area]\n",
    "cal_area_ips = cal_area_ips[cal_area_ips['Component Name'].str.contains('IDA')]\n",
    "cal_area_ips.loc[:,'Sample Name Date'] = cal_area_ips['Sample Name'].astype(str) + \"_\" + cal_area_ida['Acquisition Date & Time']\n",
    "\n",
    "# Create pivot table with Sample name as the index, component name as the column headers, and area as the values\n",
    "cal_area_ips_piv = cal_area_ips.pivot_table(\n",
    "    index=('Sample Name Date',), columns='Component Name', values='Area IPS', aggfunc='first', dropna=False,\n",
    "    )\n",
    "#establishes mean for each column\n",
    "mean_cal = cal_area_ips_piv.mean(skipna=True)\n",
    "mean_cal=pd.DataFrame(mean_cal).T\n",
    "mean_cal.index=['Average']\n",
    "cal_area_ips_piv = pd.concat([cal_area_ips_piv,mean_cal])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration IDA/IPS ratio\n",
    "cal_ida_ips_ratio = cal_area_ida_piv.loc['Average']/cal_area_ips_piv.loc['Average']\n",
    "cal_area_ips_piv.loc['Average']\n",
    "cal_area_ida_piv.loc['Average']\n",
    "#Sample IDA/IPS Ratio\n",
    "sample_ida_ips_ratio = area_ida_piv/area_ips_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate response factor by dividing sample ratio through calibration ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPS normalized recoveries to calibration data\n",
    "ips_norm_recovery = sample_ida_ips_ratio / cal_ida_ips_ratio\n",
    "# color code recoveries\n",
    "def color_map(val):\n",
    "    if in_range_min_val <= val <= in_range_max_val:\n",
    "        return in_range\n",
    "    elif val < out_range_min_val or val > out_range_max_val:\n",
    "        return out_range\n",
    "    else:\n",
    "        return question_range\n",
    "\n",
    "# Apply the style function to the entire DataFrame\n",
    "styled_ips_norm_recovery = ips_norm_recovery.style.map(color_map)\n",
    "styled_ips_norm_recovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to turn 'Sample Name Date' into a column\n",
    "calculated_recovery = ips_norm_recovery.reset_index()\n",
    "test = '240821 PB_9/5/2024 10:47'\n",
    "\n",
    "pattern = r'(.+?)_(\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{1,2}$)'\n",
    "\n",
    "# Apply the regex to extract 'Sample Name' and 'Acquisition Date & Time'\n",
    "calculated_recovery[['Sample Name', 'Acquisition Date & Time']] = calculated_recovery['Sample Name Date'].str.extract(pattern)\n",
    "\n",
    "# # Convert 'Acquisition Date & Time' to datetime format -> nice idea, should be implemented in the future, does currently not work for any type of input file\n",
    "# calculated_recovery['Acquisition Date & Time'] = pd.to_datetime(calculated_recovery['Acquisition Date & Time'], format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
    "# calculated_recovery['Acquisition Date & Time'] = calculated_recovery['Acquisition Date & Time'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Drop the original 'Sample Name Date' if no longer needed\n",
    "calculated_recovery.drop('Sample Name Date', axis=1, inplace=True)\n",
    "calculated_recovery= pd.melt(calculated_recovery, \n",
    "                      id_vars=['Sample Name', 'Acquisition Date & Time'],  # Keep these as identifier variables\n",
    "                      var_name='IDA Name',    # The new column for 'IDA name' will contain what were previously column names\n",
    "                      value_name='Calculated Recovery')     # This column will contain the values from the old 'IDA name' columns\n",
    "\n",
    "# Perform an inner merge to get only the rows that match on 'Sample Name', 'Acquisition Date & Time', and 'IDA Name'\n",
    "matching_rows_df = pd.merge(\n",
    "    quantification_pfas_default[['Sample Name', 'Acquisition Date & Time', 'IDA Name']],\n",
    "    calculated_recovery[['Sample Name', 'Acquisition Date & Time', 'IDA Name']],\n",
    "    on=['Sample Name', 'Acquisition Date & Time', 'IDA Name'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter quantification_pfas_default for rows that match in all three columns\n",
    "quantification_matching = quantification_pfas_default[\n",
    "    quantification_pfas_default[['Sample Name', 'Acquisition Date & Time', 'IDA Name']].apply(\n",
    "        tuple, axis=1\n",
    "    ).isin(matching_rows_df.apply(tuple, axis=1))\n",
    "]\n",
    "\n",
    "# Filter calculated_recovery for rows that match in all three columns\n",
    "calculated_recovery_matching = calculated_recovery[\n",
    "    calculated_recovery[['Sample Name', 'Acquisition Date & Time', 'IDA Name']].apply(\n",
    "        tuple, axis=1\n",
    "    ).isin(matching_rows_df.apply(tuple, axis=1))\n",
    "]\n",
    "# Merge the filtered calculated_recovery back into quantification_pfas_default\n",
    "quantification_pfas_default = quantification_pfas_default.merge(\n",
    "    calculated_recovery_matching[['Sample Name', 'Acquisition Date & Time', 'IDA Name', 'Calculated Recovery']],\n",
    "    on=['Sample Name', 'Acquisition Date & Time', 'IDA Name'],\n",
    "    how='left'  # Using 'left' join to keep all rows from quantification_pfas_default\n",
    ")\n",
    "# Add a new column 'Poor Recovery' based on whether 'Calculated Recovery' is outside the range\n",
    "# quantification_pfas_default.loc[:,'Poor Recovery'] = ~quantification_pfas_default['Calculated Recovery'].between(\n",
    "#     out_range_min_val, out_range_max_val, inclusive='both',\n",
    "#     )\n",
    "\n",
    "quantification_pfas_default.loc[:,'Poor Recovery'] = ~quantification_pfas_default['Recovery Rate'].between(\n",
    "    out_range_min_val * 100, out_range_max_val * 100, inclusive='both',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reported Recovery Pivot Table\n",
    "selected_columns_area = ['Sample Name', 'Sample Index','Acquisition Date & Time','Component Name', 'Reported Recovery']\n",
    "reported_recovery = quantification_blank[selected_columns_area]\n",
    "reported_recovery = reported_recovery[reported_recovery['Component Name'].str.contains('IDA')]\n",
    "reported_recovery.loc[:,'Sample Name Date'] = reported_recovery['Sample Name'].astype(str) + \"_\" + reported_recovery['Acquisition Date & Time']\n",
    "\n",
    "# Create pivot table with Sample name as the index, component name as the column headers, and area as the values\n",
    "reported_recovery_piv = reported_recovery.pivot_table(\n",
    "    index=('Sample Name Date',), columns='Component Name', values='Reported Recovery', aggfunc='first', dropna=False,\n",
    "    )\n",
    "reported_recovery_piv=reported_recovery_piv/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color map of Reported recoveries\n",
    "styled_reported_recovery = reported_recovery_piv.style.applymap(color_map)\n",
    "styled_reported_recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following file extracts quality control standard data (QSC0) from the isotope dilution analaysis (IDA) \n",
    "# and appends it to an excisting QCS0 file which is indicated in the second code block\n",
    "# (*qcs0_filepath*).\n",
    "\n",
    "# file path to write QCS0 data to \n",
    "# qcs0_filepath = r'example_data_processed\\QCS0_area_values.csv' \n",
    "\n",
    "# Filter rows where 'Sample Name' contains 'QCS0'\n",
    "#qcs0_samples = ida_area_piv.reset_index()\n",
    "#qcs0_samples = qcs0_samples[qcs0_samples['Sample Name Date'].str.contains('QCS0')]\n",
    "\n",
    "# Append the filtered DataFrame to an existing CSV file\n",
    "#if os.path.exists(qcs0_filepath):\n",
    "    # Load the existing data\n",
    "   # qsc0_existing_data = pd.read_csv(qcs0_filepath)\n",
    "    \n",
    "    # Combine existing data with new data, avoiding duplicates\n",
    "    #qsc0_combined_data = pd.concat([qsc0_existing_data, qcs0_samples]).drop_duplicates(subset=['Sample Name Date'])\n",
    "    \n",
    "    # Write back to the CSV without writing headers again\n",
    "    #qsc0_combined_data.to_csv(qcs0_filepath, index=False)\n",
    "#else:\n",
    "    # If the file doesn't exist, write the data with headers\n",
    "    #qcs0_samples.to_csv(qcs0_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute method detection limits (MDL) based on avaerage and standard deviation of process blanks. Use instrument detection limits (IDL) for the PFAS compounds not included in the process blanks. \\n\n",
    "Introduce new column 'Below Detection Threshold' and indicate all Calculated Concentration Values of PFAS quantification below the determined detection limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_only is None or blank_only.empty:\n",
    "    # Make empty dataframe if blanks for MDL calculations are not available\n",
    "    quantification_only_ = quantification_only[~quantification_only['Component Name'].str.contains('IDA|IPS|13C|MS')]\n",
    "    mdl = quantification_only_.groupby('Component Name', as_index=False)['Calculated Concentration'].sum()\n",
    "    mdl['Calculated Concentration'] = [np.nan] * len(mdl)\n",
    "    mdl['Calculated Concentration Std'] = [np.nan] * len(mdl)\n",
    "    mdl['MDL'] = [np.nan] * len(mdl)\n",
    "else:\n",
    "    # Isolate blank data and remove IDA/IPS values as well as TOF channels\n",
    "    selected_columns = ['Sample Name', 'Component Name', 'Calculated Concentration']\n",
    "    blank_only = blank_only[selected_columns]\n",
    "    blank_only = blank_only[~blank_only['Component Name'].str.contains('IDA|IPS|13C|MS')]\n",
    "\n",
    "    # create data frame with average and standard deviation values for MDL calculation and caluclate MDL\n",
    "    mdl = blank_only.groupby('Component Name', as_index=False)['Calculated Concentration'].mean()\n",
    "    mdl['Calculated Concentration Std'] = blank_only.groupby('Component Name', as_index=False)['Calculated Concentration'].std()['Calculated Concentration'].to_list()\n",
    "    mdl['MDL'] = mdl['Calculated Concentration'] + 3 * mdl['Calculated Concentration Std']\n",
    "\n",
    "# Specify column name for clarity\n",
    "mdl.rename(columns={'Calculated Concentration': 'Calculated Concentration Mean'}, inplace=True)\n",
    "\n",
    "# Load idl values from IDL_IQL file and IDL_2024 file\n",
    "idl_2024 = pd.read_csv(IDL_2024_filepath, index_col=0, low_memory=False, nrows=1)\n",
    "idl = pd.read_csv(IDL_IQL_filepath, skiprows=[2], index_col=0)\n",
    "idl = idl.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Use idl_2024 and append all values from idl, which are not included in 2024\n",
    "for idl_column in idl.columns:\n",
    "    if idl_column not in idl_2024.columns:\n",
    "        idl_2024[idl_column] = idl[idl_column]\n",
    "\n",
    "# Write each iql value in new column of mdl dataframe\n",
    "mdl.loc[:,'IDL'] = [np.nan] * len(mdl)\n",
    "for row_index in mdl.index:\n",
    "    component_name = mdl.loc[row_index, 'Component Name']\n",
    "    try:\n",
    "        mdl.loc[row_index, 'IDL'] = idl_2024[f'{component_name}'].to_list()[0]\n",
    "    except:\n",
    "        print(f'No IDL available for {component_name}')\n",
    "\n",
    "# Use component name as new index of data frame.\n",
    "mdl.index = mdl['Component Name']\n",
    "mdl.drop(columns=['Component Name'], inplace=True)\n",
    "\n",
    "mdl['Detection Threshold'] = mdl['MDL']\n",
    "mdl['Detection Threshold'].fillna(mdl.IDL, inplace=True)\n",
    "\n",
    "# write detection threshold to excel file\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl', mode='a') as writer:\n",
    "    mdl.to_excel(writer, sheet_name='Detection Threshold')\n",
    "\n",
    "# include detection threshold to long format table data quantification_pfas_default\n",
    "# assign right detection thresholds to right rows.\n",
    "quantification_pfas_default.loc[:, 'Below Detection Threshold'] = [False] * len(quantification_pfas_default)\n",
    "quantification_pfas_default.loc[:, 'Detection Threshold'] = [False] * len(quantification_pfas_default)\n",
    "for row_index in quantification_pfas_default.index:\n",
    "    component_name = quantification_pfas_default.loc[row_index, 'Component Name']\n",
    "    detection_threshold = mdl.loc[component_name, 'Detection Threshold']\n",
    "    quantification_pfas_default.loc[row_index, 'Detection Threshold'] = detection_threshold\n",
    "    if quantification_pfas_default.loc[row_index, 'Calculated Concentration'] < detection_threshold:\n",
    "        quantification_pfas_default.loc[row_index, 'Below Detection Threshold'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate ratio of calculated concentration from default channel and _TOF MS channel for all PFAS components and all samples.\n",
    "Detect all calculated concentration values which deviate more than 30 % between channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign TOF channel to each PFAS and save calculated concentration to new column\n",
    "quantification_pfas_default.loc[:,'Calculated Concentration TOF'] = [np.nan] * len(quantification_pfas_default)\n",
    "for row_index in quantification_pfas_default.index:\n",
    "    sample_index = quantification_pfas_default.loc[row_index, 'Sample Index']\n",
    "    tof_channel_name = quantification_pfas_default.loc[row_index, 'Component Name'] + '_TOF MS'\n",
    "    tof_channel_row = quantification_only[(\n",
    "        (quantification_only['Component Name'] == tof_channel_name) &\n",
    "        (quantification_only['Sample Index'] == sample_index)\n",
    "    )]\n",
    "    quantification_pfas_default.loc[row_index, 'Calculated Concentration TOF'] = \\\n",
    "        tof_channel_row.loc[:,'Calculated Concentration'].to_list()[0]\n",
    "\n",
    "# Calculate average deviation of the channels\n",
    "quantification_pfas_default['Channel Ratio'] = (\n",
    "    200 * (quantification_pfas_default['Calculated Concentration'] - quantification_pfas_default['Calculated Concentration TOF']) \\\n",
    "        / (quantification_pfas_default['Calculated Concentration'] + quantification_pfas_default['Calculated Concentration TOF'])\n",
    ").round(decimals=1)\n",
    "\n",
    "# Introduce new column where everything below method detection limit is marked\n",
    "quantification_pfas_default[f'Channel Ratio > {allowed_channel_deviation}'] = abs(quantification_pfas_default['Channel Ratio']) > allowed_channel_deviation\n",
    "\n",
    "# Create pivot table\n",
    "channel_ratio = quantification_pfas_default.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values='Channel Ratio', aggfunc='first', dropna=False,\n",
    ")\n",
    "\n",
    "# color code percentage deviation\n",
    "def deviation_color_map(val):\n",
    "    if val is np.nan:\n",
    "        return\n",
    "    if allowed_channel_deviation * (-1) <= val <= allowed_channel_deviation:\n",
    "        return in_range\n",
    "    elif val < allowed_channel_deviation * (-1) or val > allowed_channel_deviation:\n",
    "        return out_range\n",
    "\n",
    "channel_ratio = channel_ratio.style.map(deviation_color_map)\n",
    "\n",
    "# write to existing excel file\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl', mode='a') as writer:\n",
    "    channel_ratio.to_excel(writer, sheet_name='Channel Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write final concentration table with all information to excel. Use concentration value, indicate everything below detection threshold with '< MDL', and indicate everything with more than 30 % deviation between challes with '> CR' /\n",
    "In addition, concentration values are converted to ng\\g or ng\\l respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_values(data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Flags column values (most probably concentrations) with channel ratio, recovery rates and MDLs.\n",
    "\n",
    "    :param data: data frame containing columns 'Channel Ratio', 'Poor Recovery', and 'Below Detection Threshold',\n",
    "    as well as the column you indicated.\n",
    "    :type data: pd.DataFrame\n",
    "    :param column: Colun name of data frame to be filtered or flagged.\n",
    "    :type column: str\n",
    "    :return: Data frame, where the column data is flagged.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"    \n",
    "    final_table = data[['Sample Name', 'Component Name', column]]\n",
    "    final_table.loc[final_table.index[quantification_pfas_default[f'Channel Ratio > {allowed_channel_deviation}']], column] = '> CR'\n",
    "    final_table.loc[final_table.index[quantification_pfas_default['Poor Recovery']], column] = 'Poor Recovery'\n",
    "    final_table.loc[final_table.index[quantification_pfas_default['Below Detection Threshold']], column] = '< MDL'\n",
    "    return final_table\n",
    "\n",
    "# Transform concentration to ng/g or ng/l, depending on your sample_unit\n",
    "quantification_pfas_default[f'Concentration in ng per {sample_unit}'] = quantification_pfas_default['Calculated Concentration'] / nonextracted_sample_quantity\n",
    "\n",
    "# Flag concentration values with channel ratio, recovery rates and detection threshold.\n",
    "calculated_concentration = flag_values(data=quantification_pfas_default, column='Calculated Concentration')\n",
    "calculated_concentration_II = flag_values(data=quantification_pfas_default, column=f'Concentration in ng per {sample_unit}')\n",
    "\n",
    "# Pivot concentration tables.\n",
    "calculated_concentration = calculated_concentration.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values='Calculated Concentration', aggfunc='first', dropna=False,\n",
    ")\n",
    "\n",
    "calculated_concentration_II = calculated_concentration_II.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values=f'Concentration in ng per {sample_unit}', aggfunc='first', dropna=False,\n",
    ")\n",
    "\n",
    "# Write pivot tables to existing excel file\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl', mode='a') as writer:\n",
    "    calculated_concentration.to_excel(writer, sheet_name='Concentration Table')\n",
    "    if sample_unit == \"l\":\n",
    "        calculated_concentration_II.to_excel(writer, sheet_name=f'Concentration (ng per l)')\n",
    "    else:\n",
    "        calculated_concentration_II.to_excel(writer, sheet_name=f'Concentration (ng per g)')\n",
    "\n",
    "# Write long format data to csv\n",
    "quantification_pfas_default = quantification_pfas_default[[\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time','Component Name', 'Area', 'Calculated Concentration',\n",
    "    f'Concentration in ng per {sample_unit}', 'IDA Name', 'IDA Area', 'IDA Concentration', 'IPS Name', 'IPS Area', 'IPS Concentration',\n",
    "    'Recovery Rate', 'Recovery Rate Uncertainty 1', 'Recovery Rate Sciex', 'Detection Threshold', 'Below Detection Threshold',\n",
    "    'Channel Ratio', f'Channel Ratio > {allowed_channel_deviation}','Calculated Recovery','Poor Recovery'\n",
    "]]\n",
    "\n",
    "quantification_pfas_default.to_csv(processed_filepath_long)\n",
    "\n",
    "# Print output\n",
    "calculated_concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine linear calibration curve based on calibration data.\n",
    "*to be moved somewhere else, not relevant at the moment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sanitize file names\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Removes special characters from PFAS name to create valid directory names.\"\"\"    \n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n",
    "\n",
    "# extract PFAS data\n",
    "selected_columns = ['Sample Name', 'Component Name', 'Actual Concentration','IS Actual Concentration','Area','IS Area', 'Used']\n",
    "calibration_only = calibration_only[selected_columns]\n",
    "calibration_only = calibration_only[~calibration_only['Component Name'].str.contains('IDA|IPS|13C|MS')]\n",
    "calibration_only['Concentration/IS Concentration'] = calibration_only['Actual Concentration']/calibration_only['IS Actual Concentration']\n",
    "calibration_only['Area/IS Area'] = calibration_only['Area']/calibration_only['IS Area']\n",
    "calibration_only = calibration_only.loc[calibration_only['Used'], :]\n",
    "# calibration_only.loc[calibration_only['Area/IS Area'].isnull(), :] = 0\n",
    "calibration_only['Area/IS Area'].replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "# Create a list of unique sample names and count them.\n",
    "components = calibration_only['Component Name'].unique()\n",
    "\n",
    "image_paths = []\n",
    "calibration_coefficients = {'slope': {}, 'intercept':{}}\n",
    "# Iterate over each component and create scatter plots with regression lines\n",
    "for i, component in enumerate(components):\n",
    "    component_data = calibration_only[calibration_only['Component Name'] == component]\n",
    "    \n",
    "    # Extract x and y values\n",
    "    x = component_data['Concentration/IS Concentration'].values.reshape(-1, 1).flatten()\n",
    "    y = component_data['Area/IS Area'].values\n",
    "    y_forced = [0 if x_elem==0 else y_elem for (x_elem, y_elem) in zip(x, y)]  # force calibration curve through zero\n",
    "\n",
    "    # log values\n",
    "    # x = [1e-3 if elem == 0 else ma.log10(elem) for elem in x]\n",
    "    # y_forced = [1e-3 if elem == 0 else ma.log10(elem) for elem in y_forced]\n",
    "\n",
    "    weights = []\n",
    "    for elem in x:\n",
    "        if elem == 0:\n",
    "            weights.append(1e3)\n",
    "        else:\n",
    "            weights.append(1/elem)\n",
    "    if all(elem==1e3 for elem in weights):\n",
    "        print(f'All calibration data for {component_name} in NaN. Therefor component is skipped')\n",
    "        continue\n",
    "\n",
    "    # perform linear regression with numpy and weights\n",
    "    numpy_model = np.polyfit(x=x, y=y_forced, deg=1, w=weights)\n",
    "\n",
    "    slope = numpy_model[0]\n",
    "    intercept = numpy_model[1]\n",
    "\n",
    "    # fit values, and mean\n",
    "    ypred = [numpy_model[0] * elem + numpy_model[1] for elem in x]                        # or [p(z) for z in x]\n",
    "    ybar = np.sum(y)/len(y)          # or sum(y)/len(y)\n",
    "    ssreg = np.sum((ypred-ybar)**2)   # or sum([ (yihat - ybar)**2 for yihat in yhat])\n",
    "    sstot = np.sum((y - ybar)**2)    # or sum([ (yi - ybar)**2 for yi in y])\n",
    "    r2 = ssreg / sstot\n",
    "    \n",
    "    # save calibration coefficient to dictionary\n",
    "    calibration_coefficients['slope'][component] = slope\n",
    "    calibration_coefficients['intercept'][component] = intercept\n",
    "\n",
    "    # Regression equation\n",
    "    equation = f'y = {slope:.2f}x + {intercept:.2f}'\n",
    "\n",
    "    plt.figure(figsize = (8,6))\n",
    "    # Plot with Seaborn\n",
    "    sns.regplot(\n",
    "       # ax=axes[i], \n",
    "        x=x, \n",
    "        y=y, \n",
    "        scatter=True, \n",
    "        fit_reg=True,\n",
    "        line_kws={\"color\": \"red\"},  # Color of the regression line\n",
    "        scatter_kws={\"s\": 50, \"alpha\": 0.7},  # Customize scatter points\n",
    "        ci=95\n",
    "    )\n",
    "    plt.plot(x, ypred)\n",
    "    # Set the title with the component name\n",
    "    plt.title(f'{component}')\n",
    "    plt.ylabel('Concentration/IS Concentration')\n",
    "    plt.xlabel('Area/IS Area')\n",
    "    plt.text(0.05, 0.95, f'{equation}\\n$R^2$ = {r2:.2f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             fontsize=10, \n",
    "             verticalalignment='top', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "    plt.show()\n",
    "   \n",
    "    # Sanitize the file name\n",
    "    sanitized_component = sanitize_filename(component)\n",
    "    image_path = os.path.join(plot_directory, f'{sanitized_component}.png')\n",
    "    \n",
    "    # Save the plot as an image\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "    \n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare calibration with sciex data and print S\n",
    "i = 0\n",
    "for (_, row) in quantification_pfas_default.iterrows():\n",
    "    try:\n",
    "        coef_k = calibration_coefficients['slope'][row['Component Name']]\n",
    "        coef_d = calibration_coefficients['intercept'][row['Component Name']]\n",
    "    except:\n",
    "        print(f'Calibration curve for {component} does not excist')\n",
    "        continue\n",
    "    concentration_sciex = row['Calculated Concentration']\n",
    "    concentration_from_scratch = (row['Area'] / row['IDA Area'] - coef_d) * row['IDA Concentration'] / coef_k\n",
    "    if concentration_sciex == concentration_sciex:\n",
    "        percentage_deviation = round(\n",
    "            200 * (concentration_sciex - concentration_from_scratch)/(concentration_sciex + concentration_from_scratch), 1\n",
    "            )\n",
    "        if abs(percentage_deviation) > 5:\n",
    "            print(i, row['Component Name'], percentage_deviation)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes all relevant data to excel file and adds calibration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create excel with pandas excelwriter\n",
    "with pd.ExcelWriter(processed_filepath, engine='openpyxl', mode='a') as writer:\n",
    "    styled_ips_norm_recovery.to_excel(writer, sheet_name = 'Calculated Recoveries')\n",
    "    styled_reported_recovery.to_excel(writer, sheet_name = 'Reported Recoveries')\n",
    "\n",
    "workbook = load_workbook(processed_filepath)\n",
    "plot_sheet = workbook.create_sheet('Calibration Curves')\n",
    "    \n",
    "# Insert all images into one sheet in a grid format\n",
    "row_offset = 1  # Start at the first row\n",
    "col_offset = 1  # Start at the first column\n",
    "images_per_row = 2  # Number of images per row\n",
    "\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    # Calculate the position for each image\n",
    "    row_position = row_offset + (i // images_per_row) * 15  # Adjust the multiplier to control spacing\n",
    "    col_position = col_offset + (i % images_per_row) * 20   # Adjust the multiplier to control spacing\n",
    "    \n",
    "    # Load the image\n",
    "    img = Image(image_path)\n",
    "    \n",
    "    # Place the image at the calculated position\n",
    "    cell_position = plot_sheet.cell(row=row_position, column=col_position).coordinate\n",
    "    plot_sheet.add_image(img, cell_position)\n",
    "\n",
    "workbook.save(processed_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lohmann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

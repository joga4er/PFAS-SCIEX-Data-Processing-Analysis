{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ai0Ccju5rTr"
   },
   "source": [
    "name: 241114_data_analysis \\\n",
    "date: 11/14/2024 \\\n",
    "version: 2.0 \\\n",
    "github root: #11 \\\n",
    "author: Justin Sankey, Johanna Ganglbauer \\\n",
    "\n",
    "**description**: Takes raw liquit chromatography mass spectroscopy (LCMS) data (exported table from SCIEX Analyst Software), computes recovery rates, method detection limits, and ratios of default channel to MS TOF channel. Generates plots and writes results to excel and creates long format table (.csv) for data publications.\n",
    "\n",
    "**changes in comparison to previous version**:\n",
    "*   inputs through files: when the code is run for the first time it creates an empty .csv file containing all your sample names as rows. You should indicate your sample volume/weight, unit and if the sample is considered for the mdl calculation.\n",
    "The same holds true for the threshold of your recovery rates per extracted standard.\n",
    "*   order/mass tables: in order to facilitate the selection of samples for the mdl evaluation the table of detected PFAS masses as well as a plot with the PFAS concentrations per sample is created in the beginning.\n",
    "*   final concentration per ml or g is evluated for each sample separately based on new inputs.\n",
    "*   recovery rates: each extracted standard (IDA) has individual limits for the allowed recovery range. If both core and extended method are available, you can know choose to use (i) only the recovery rates of the core method, (ii) only the recovery rates of the extended method, (iii) the average of recovery rates of core and extended method\n",
    "*   channel ratios: are also evaluated for standards. Are set to NaN if one of the channels (TOF MS or MS/MS is not included in either method)\n",
    "\n",
    "**what needs to be implemented**:\n",
    "*   JB: read in different IDL files depending on sample matrix\n",
    "*   ...\n",
    "*   ...\n",
    "*   **feel free to add your thoughts!**\n",
    "\n",
    "**contact/help/complaints:** johanna.ganglbauer@uri.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aYFHWVAC8Q8"
   },
   "source": [
    "# Specifying Inputs - Loading files and packages\n",
    "The block below will load all python packages needed for the following analysis.\n",
    "\n",
    "**Uncomment the first code block if you are using google colab and want to connect to the cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37Q4XbjpA1ze"
   },
   "outputs": [],
   "source": [
    "# connect google colab with your google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import all needed packages\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pathlib as pali\n",
    "\n",
    "# import pandas and suppress warnings\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AfPiSd_58H5"
   },
   "source": [
    "**Enter in your input filepaths:**\n",
    "\n",
    "(1) raw_data_filepaths is a list of files pointing to the exported table from the Sciex Analyst Software you want to analyze. Each file representing results from the core method must end with '_core', each file representing results from the extended method must end with '_extended'. \\\n",
    "(2) idl_filepath is a .csv file containing instrumentation detection limits (IDLs)\n",
    "\n",
    "You can copy the filepath of each file on windows by right clicking on the file in your file explorer and choosing the option *Copy as Path*. \\\n",
    "The same works on Mac OS: Control-click or right-click on the file in Finder. Press the Option (Alt) key. Choose *Copy [filename] as Pathname*.\n",
    "\n",
    "**Enter in your output filepaths:** \\\n",
    "When calling the script for the first time, two csv tables will be created which assist you in providing all the input parameters: \\\n",
    "(1) sample_input_parameter_filepath: contains a list of all samples in your input data and empty fields for you to provide the original sample volume or weight and allows you to decide if the sample should be considered for the evaluation of method detection limits. In the variable below you indicate where the sample_input_file should be saved to. Once the input file was created it will not be changed by the script. \\\n",
    "(2) threshold_input_parameter_filepath: contains a list of all injection standards and allows you to provide the valid range of recovery rates. In the variable below you indicate where the threshold_input_parameter should be saved to. Once the input file was created it will not be changed by the script.\n",
    "\n",
    "Finally, the script will create a .csv file containing the most relevant data, as well as an excel file containing the most relevant results of the evaluation.\\\n",
    "(1) The variable processed_filepath indicates location and file prefix where your results should be saved to. \\\n",
    "(2) The variable plot_directory indictes location where your plots should be saved to.\n",
    "\n",
    "**Make sure all your input and output filepaths are within apostrophes and start with an r. (r'...')** \\\n",
    "The appostrophes make python understand the variable is a string (text) and not a number.\\\n",
    "The r in front of the string (raw string) makes python interpret the backslashes in the file path correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZozFL6t7sax"
   },
   "outputs": [],
   "source": [
    "### INPUT FILEPATHS\n",
    "# raw data upload file path\n",
    "raw_filepaths = [\n",
    "    r'example_data_raw\\20240920_PFAS_Standard_Check_Updated_core.txt',\n",
    "]\n",
    "\n",
    "# file paths for IDL and IQL data - not meant to be adopted\n",
    "idl_filepath = r'example_data_raw/IDL_2024.csv'\n",
    "\n",
    "### OUTPUT FILLEPATHS\n",
    "# input parameter filepath\n",
    "sample_input_parameter_filepath = r'sample_input_parameter.csv'\n",
    "threshold_input_parameter_filepath = r'threshold_input_parameter.csv'\n",
    "\n",
    "# processed data output excel/csv file path - without .xlsx/.csv ending\n",
    "processed_filepath =r'example_data_processed/test_data_justin_check'\n",
    "\n",
    "# directory to save plots to\n",
    "plot_directory = r'example_figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewIeK7Ak7v1_"
   },
   "source": [
    "**Enter in your parameters:**\n",
    "\n",
    "(1) Indicate the maximal allowed percentage deviation between MS TOF channel and MS/MS channel. \\\n",
    "(2) Provide a default value for the instrumentation detection limit (IDL) in case the component is not listed in the csv. \\\n",
    "(3) Indicate which data you want to use to determine recovery rates: \\\n",
    "    (i)'core': only data from core method is used to determine recovery rates. This means peak integration on non-extracted standards (IPS) must be done for core method only. \\\n",
    "    (ii) 'extended': only data from from extended methode is used to determine recovery rates. This means peak integration of non-extracted standards (IPS) must be done for extended method only. \\\n",
    "    (iii) 'average': the average peak areas from core method and extended methods are used to determine recovery rates. This means peak integration of IPS peaks must be done for both core method and extended method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT4UCiwFF_b0"
   },
   "outputs": [],
   "source": [
    "# threshold for acceptance of absolute percentage difference between default channel and TOF MS channel\n",
    "allowed_channel_deviation = 30\n",
    "\n",
    "# idl default value\n",
    "idl_unknown = 1e-3\n",
    "\n",
    "# data to determine recovery rates:\n",
    "# available options: 'core', 'extended' and 'average'\n",
    "recovery_data = 'core'\n",
    "\n",
    "# sanity check - will produce an error if you indicated a variable recovery_data which is not allowed.\n",
    "if recovery_data not in ['core', 'extended', 'average']:\n",
    "    raise Exception(\"\"\"\n",
    "        Invalid input for variable recovery_data, use either 'core', 'extended', or 'average'.\n",
    "                    \"\"\")\n",
    "\n",
    "# will be used to identify standards by the column 'Component Name'\n",
    "standard_identifiers = 'IDA|IPS|13C|d-|d3-|d5-|18O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBdL-aTpGFpY"
   },
   "source": [
    "The following code block makes sure that the output folders exist and reads in data. Moreover, it sets the display options to show data tables in your console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H77BcO6GO4M"
   },
   "outputs": [],
   "source": [
    "# Ensure file path and folder path exist to write outputs to and create folders, if they do not exist\n",
    "folder_path = os.path.dirname(processed_filepath)\n",
    "if folder_path and not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "processed_filepath_xlsx = processed_filepath + '.xlsx'\n",
    "processed_filepath_csv = processed_filepath + '.csv'\n",
    "\n",
    "if not os.path.exists(plot_directory):\n",
    "    os.makedirs(plot_directory)\n",
    "\n",
    "# Define columns of input which are needed for further processes:\n",
    "columns_considered = [\n",
    "    'Sample Name', 'Sample Index', 'Sample Comment', 'Sample Type',\n",
    "    'Component Name',  'Component Group Name', 'Component Comment', 'IS Name',\n",
    "    'Acquisition Date & Time', 'Injection Volume', 'Used',\n",
    "    'Calculated Concentration', 'Actual Concentration',\n",
    "    'Reported Recovery', 'IDA Average Response Factor',\n",
    "    'Area',\n",
    "]\n",
    "\n",
    "# Load input data files and put them all in one dataframe\n",
    "data = pd.DataFrame()  # initialize empty data frames\n",
    "sample_index = 0  # initialize Component Index\n",
    "for file in raw_filepaths:\n",
    "    # read in file\n",
    "    if file[-4:] == '.csv':\n",
    "        this_data = pd.read_csv(file, delimiter=',', encoding='utf-8', low_memory=False, header=0,)\n",
    "    elif file[-4:] == '.txt':\n",
    "        this_data = pd.read_csv(file, delimiter='\\t', encoding='utf-8', low_memory=False, header=0,)\n",
    "    else:\n",
    "        raise ImportError('Raw input file paths must either be .csv or .txt files.')\n",
    "\n",
    "    # increase component index to remain unique for multiple data frames\n",
    "    this_data['Sample Index'] = this_data['Sample Index'] + sample_index\n",
    "    sample_index += max(this_data['Sample Index'])\n",
    "    check = this_data['Sample Index'].value_counts()\n",
    "\n",
    "    # make sure each sample name ends with Ext for extended method and with Core for core method\n",
    "    if file[-12:-4] == 'extended':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Ext')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Ext' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 108)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "\n",
    "    elif file[-8:-4] == 'core':\n",
    "        mask_names = this_data['Sample Name'].str.endswith('Core')\n",
    "        this_data['Sample Name'][~mask_names] = [compound + ' Core' for compound in this_data['Sample Name'][~mask_names].to_list()]\n",
    "\n",
    "        # checks if extended data has 107 channels\n",
    "        for index, elem in check[(check != 142)].items():\n",
    "            print(f'The sample with Index {index} has {elem} channels.')\n",
    "    else:\n",
    "        print('If you combine core method and extended method make sure your input file names end with _core and _extended respectively.')\n",
    "\n",
    "    # append actual dataframe in list (this_data) to huge dataframe (data)\n",
    "    if data.empty:\n",
    "        data = this_data[columns_considered]  # initialize data in first step (when data is empty)\n",
    "    else:\n",
    "        data = pd.concat([data, this_data[columns_considered]], ignore_index=True)  # append to data\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full width of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Code block cleans up data: \\\n",
    " (i) replace strange strings with NaN, \\\n",
    " (ii) make sure the component IPS-13C2_PFOA is not used as non extracted standard, replace it with IPS-13C4_PFOA, \\\n",
    " (iii) make sure all _TOF MS channel names are labelled right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up 'Calculated Concentration' column - set all strange strings to NaN\n",
    "data['Calculated Concentration'] = data['Calculated Concentration'].replace(\n",
    "    {'<1 points': np.nan, '< 0': np.nan, 'no root': np.nan, 'NaN': np.nan}\n",
    "    ).astype('float')\n",
    "\n",
    "# Replace 'IPS-13C2_PFOA' values: optional, only when component name occurs\n",
    "if any(data['Component Name'].isin(['IPS-13C4_PFOA'])):\n",
    "    data['Component Group Name'] = data['Component Group Name'].replace('IPS-13C2_PFOA', 'IPS-13C4_PFOA')\n",
    "\n",
    "    # Find rows where 'Component Group Name' is 'IPS-13C4_PFOA' (after replacement)\n",
    "    mask = data['Component Group Name'] == 'IPS-13C4_PFOA'\n",
    "\n",
    "    # Iterate through each of these rows and replace area in column\n",
    "    for idx, row in data[mask].iterrows():\n",
    "        sample_name = row['Sample Name']\n",
    "\n",
    "        # Find the corresponding row with 'Component Name' == 'IPS-13C4_PFOA' and the same 'Sample Name'\n",
    "        matching_row = data[(data['Component Name'] == 'IPS-13C4_PFOA') & (data['Sample Name'] == sample_name)]\n",
    "\n",
    "        if not matching_row.empty:\n",
    "            # Update the 'Area IPS' with the value from 'Area' in the matching row\n",
    "            data.at[idx, 'Area IPS'] = matching_row['Area'].values[0]\n",
    "\n",
    "# Correct channel names in original data (all of the TOF channels are labelled by _TOF MS, only 2 of them are labeled by only _TOF)\n",
    "mask_names = data['Component Name'].str.endswith('_TOF')\n",
    "data['Component Name'][mask_names] = [compound + ' MS' for compound in data['Component Name'][mask_names].to_list()]\n",
    "\n",
    "# some have an underscore between TOF and MS, this is removed\n",
    "mask_names = data['Component Name'].str.endswith('_TOF_MS')\n",
    "data['Component Name'][mask_names] = [compound[:-3] + ' MS' for compound in data['Component Name'][mask_names].to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block combines samples fromt the extended method and the core method and gives them the same sample index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sample index equal if one sample exist as both \"Core Method\" and \"Extended Method\"\n",
    "sample_names = data['Sample Name'].unique()\n",
    "\n",
    "# combine indices of Core Method and Extended Method if both are available\n",
    "detect = 0\n",
    "index_mapper = {}\n",
    "one_method_only_index = []\n",
    "# loop over unique sample names\n",
    "for sample_name in sample_names:\n",
    "    # if sample name exists for the related core method:\n",
    "    # search related core samples\n",
    "    # reset index of the core samples\n",
    "    # make sure the (index: sample name tuple is saved to index mapper)\n",
    "    # account for exceptions: more or less samples of core method than of extended one.\n",
    "    if sample_name[-3:] == 'Ext':\n",
    "        if sample_name[:-3] + 'Core' in sample_names:\n",
    "            sample_indices_core = data.loc[data['Sample Name'].isin([sample_name[:-3] + 'Core']), 'Sample Index'].value_counts().index\n",
    "            sample_indices_extended = data.loc[data['Sample Name'].isin([sample_name]), 'Sample Index'].value_counts().index\n",
    "            for index_core, index_extended in zip(sample_indices_core, sample_indices_extended):\n",
    "                data.loc[data['Sample Index'].isin([index_core, index_extended]), 'Sample Index'] = index_core\n",
    "                index_mapper[index_core] = sample_name[:-3]\n",
    "                detect+=1\n",
    "\n",
    "            if len(sample_indices_core) < len(sample_indices_extended):\n",
    "                print(f'At least one of the sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "                for index_extended in sample_indices_extended[len(sample_indices_core):]:\n",
    "                    index_mapper[index_extended] = sample_name\n",
    "                    one_method_only_index.append(index_extended)\n",
    "                    detect+=1\n",
    "\n",
    "            elif len(sample_indices_core) > len(sample_indices_extended):\n",
    "                print(f'At least one of the sample {sample_name} has no related extended Method. Find out if this is a problem.')\n",
    "                for index_core in sample_indices_core[len(sample_indices_extended):]:\n",
    "                    index_mapper[index_core] = sample_name\n",
    "                    one_method_only_index.append(index_core)\n",
    "                    detect+=1\n",
    "        else:\n",
    "            print(f'The sample {sample_name} has no related Core Method. Find out if this is a problem.')\n",
    "            sample_indices_extended = data.loc[data['Sample Name'].isin([sample_name]), 'Sample Index'].value_counts().index\n",
    "            for index_extended in sample_indices_extended:\n",
    "                index_mapper[index_extended] = sample_name\n",
    "                one_method_only_index.append(index_extended)\n",
    "                detect+=1\n",
    "\n",
    "    # catch all samples from the methods, which do not have an extended one.\n",
    "    elif sample_name[-4:] == 'Core':\n",
    "        if not sample_name[:-4] + 'Ext' in sample_names:\n",
    "            print(f'The sample {sample_name} has no related Extended Method. Find out if this is a problem.')\n",
    "            sample_indices_core = data.loc[data['Sample Name'].isin([sample_name]), 'Sample Index'].value_counts().index\n",
    "            for index_core in sample_indices_core:\n",
    "                index_mapper[index_core] = sample_name\n",
    "                one_method_only_index.append(index_core)\n",
    "                detect+=1\n",
    "    else:\n",
    "        print(f'Make sure your input data name ends either with _core or with _extended.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block the order of components is conserved and split to default channel (MS/MS) screening fragmented masses, and TOF channel screening precursor masses.\n",
    "If either channel is not available it is set to nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract channel names from first sample and separate them into standards and non-standards\n",
    "first_sample_id = data['Sample Index'].value_counts().index[0]  # index of first sample\n",
    "components_filtered = data.loc[data['Sample Index'] == first_sample_id, 'Component Name']  # channel names of first sample\n",
    "components_sorted = components_filtered[~components_filtered.str.contains(standard_identifiers)].to_list()  # channel names excluding IPS and IDA\n",
    "ida_ips_sorted = components_filtered[components_filtered.str.contains(standard_identifiers)].to_list()  # channel names excluding IPS and IDA\n",
    "\n",
    "# initialize and fill lists of sorted components\n",
    "components_fragmented = []\n",
    "components_precursor = []\n",
    "skip_components = []\n",
    "for component in components_sorted:\n",
    "    if component in skip_components:\n",
    "        continue\n",
    "    if '_TOF MS' in component:\n",
    "        components_precursor.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component[:-7] in components_sorted:\n",
    "            components_fragmented.append(component[:-7])\n",
    "        else:\n",
    "            components_fragmented.append(np.nan)\n",
    "        skip_components.append(component[:-7])\n",
    "    else:\n",
    "        components_fragmented.append(component)\n",
    "        skip_components.append(component)\n",
    "        if component + '_TOF MS' in components_sorted:\n",
    "            components_precursor.append(component + '_TOF MS')\n",
    "        else:\n",
    "            components_precursor.append(np.nan)\n",
    "        skip_components.append(component + '_TOF MS')\n",
    "\n",
    "# initialize and fill lists of sorted internal standards\n",
    "ida_ips_fragmented = []\n",
    "ida_ips_precursor = []\n",
    "skip_standards = []\n",
    "for standard in ida_ips_sorted:\n",
    "    if standard in skip_standards:\n",
    "        continue\n",
    "    if '_TOF MS' not in standard:\n",
    "        ida_ips_fragmented.append(standard)\n",
    "        skip_standards.append(standard)\n",
    "        if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard[4:] + '_TOF MS')\n",
    "            skip_standards.append(standard[4:] + '_TOF MS')\n",
    "        else:\n",
    "            ida_ips_precursor.append(np.nan)\n",
    "    else:\n",
    "        if 'IDA-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IDA-' + standard[:-7])\n",
    "                skip_standards.append('IDA-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        elif 'IPS-' + standard[:-7] in ida_ips_sorted:\n",
    "            ida_ips_precursor.append(standard)\n",
    "            skip_standards.append(standard)\n",
    "            if standard[4:] + '_TOF MS' in ida_ips_sorted:\n",
    "                ida_ips_fragmented.append('IPS-' + standard[:-7])\n",
    "                skip_standards.append('IPS-' + standard[:-7])\n",
    "            else:\n",
    "                ida_ips_fragmented.append(np.nan)\n",
    "        else:\n",
    "            print(f'The standard: {standard} has no corresponding IDA or IPS in the default MS channel. It is ignored in the following calculations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgNZioHzGV1b"
   },
   "source": [
    "The following code block separates data in calibration data, and quantification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cez1yIkYGhWg"
   },
   "outputs": [],
   "source": [
    "# Split data into quantification data, calibration data, and blanks for mdl calculation\n",
    "calibration_only = data[(data['Sample Type'] == 'Standard')]\n",
    "quantification_blank = data[(data['Sample Type'] != 'Standard')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section a table with concentrations (masses) of PFAS for all samples and all analyzed PFAS components will be created and shown as well as a plot which shows the concentrations (masses).\n",
    "\n",
    "Note: The LCMS quantifies ng/sample which is commonly referred to as concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of np.nan component names when used as index to create tables\n",
    "pfas_components = [component for component in components_fragmented if str(component) != 'nan']\n",
    "\n",
    "pfas_mass = quantification_blank.pivot_table(\n",
    "    index=('Sample Index'), columns='Component Name', values='Calculated Concentration', aggfunc='first', dropna=False,\n",
    ")\n",
    "pfas_mass.rename(index=index_mapper, inplace=True)\n",
    "pfas_mass = pfas_mass[pfas_components]\n",
    "\n",
    "# write initial concentrations to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl') as writer:\n",
    "    pfas_mass.to_excel(writer, sheet_name='Calculated Concentrations in ng')\n",
    "\n",
    "cmap = plt.cm.YlGnBu(np.linspace(0, 1, len(pfas_components)))\n",
    "fig, ax = plt.subplots()\n",
    "pfas_mass.plot.bar(stacked=True, ax=ax, color=cmap)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('PFAS [ng/sample]')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, 2.1))\n",
    "fig.savefig(\n",
    "    os.path.join(plot_directory, 'concentrations_iteration0.png'), bbox_inches='tight'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# save IPS area comparison plot to excel file\n",
    "workbook = load_workbook(processed_filepath_xlsx)\n",
    "plot_sheet = workbook.create_sheet('Details Concentrations')\n",
    "\n",
    "img = Image(os.path.join(plot_directory, 'concentrations_iteration0.png'))\n",
    "\n",
    "cell_position = plot_sheet.cell(row=1, column=1).coordinate\n",
    "plot_sheet.add_image(img, cell_position)\n",
    "\n",
    "workbook.save(processed_filepath_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block an empty csv table with a list of samples will be created for you to input the sample volume/weight, the extracted volume and select the samples considered for your mdl calculation.\\\n",
    "In addition, an empty csv table with a list of extracted standards will be created for you to input the accepted range for the recovery rates.\n",
    "\n",
    "**After the code was run for the first time, input all your parameters to the two files, save them and run the code again.**\n",
    "Once the input files are created, they won't be changed by the script anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pali.Path(sample_input_parameter_filepath).exists():\n",
    "    sample_input_data = pd.DataFrame(columns=[\n",
    "        'sample name', 'volume/weight', 'unit [ml or g]', 'extracted sample volume [ml]', 'used for mdl calculation'\n",
    "        ],\n",
    "        index=quantification_blank['Sample Index'].unique()\n",
    "        )\n",
    "    sample_input_data['sample name'] = [index_mapper[sample_index] for sample_index in sample_input_data.index]\n",
    "    sample_input_data['used for mdl calculation'] = False\n",
    "    sample_input_data['extracted sample volume [ml]'] = 0.5\n",
    "    sample_input_data.to_csv(sample_input_parameter_filepath)\n",
    "    del sample_input_data\n",
    "    raise ImportError(\n",
    "        f'The file {sample_input_parameter_filepath} was created for the first time.' + \\\n",
    "        'Adopt your sample quantities and units and select the samples you want to use for the evaluation of the method detection limit.'\n",
    "    )\n",
    "\n",
    "if not pali.Path(threshold_input_parameter_filepath).exists():\n",
    "    threshold_input_data = pd.DataFrame(columns=[\n",
    "        'lower threshold for recoveries [%]', 'upper threshold for recoveries [%]'\n",
    "        ],\n",
    "        index=[ida for ida in ida_ips_fragmented if 'IDA' in ida]\n",
    "        )\n",
    "    threshold_input_data['lower threshold for recoveries [%]'] = 50\n",
    "    threshold_input_data['upper threshold for recoveries [%]'] = 150\n",
    "    threshold_input_data.to_csv(threshold_input_parameter_filepath)\n",
    "    del threshold_input_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block loads your input parameters and input thresholds. \\\n",
    "**Make sure you input all the information in the provided input files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_data = pd.read_csv(sample_input_parameter_filepath, index_col=[0])\n",
    "recovery_thresholds = pd.read_csv(threshold_input_parameter_filepath, index_col=[0])\n",
    "recovery_thresholds_dict = recovery_thresholds.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block the area of IPS in the calibration data is compared to the area of IPS in the batch data.\n",
    "\n",
    "**Doublecheck that the information you provided in the \"Actual Concentration\" column considering the amount of IPS and IDA you added to your samples is correct! This will affect your recovery rates**.\n",
    "\n",
    "Note: the \"Actual Concentration\" column provides information on how much ng of a component you added per sample.\\\n",
    "Within the calibration 4 ng non-extracted standard (=IPS) are added per ml. The lab samples commonly have a volume of 0.5 ml.\n",
    "If you pipetted 4 ng non-extracted standard (=IPS) to your samples, the IPS areas should be twice as large when compared to the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLfcCxunG9sH"
   },
   "outputs": [],
   "source": [
    "extracted_sample_volume = sample_input_data['extracted sample volume [ml]'].mean()\n",
    "\n",
    "# Calculate IPS average area per compound in calibration data and plot it\n",
    "calibration_only_ips = calibration_only[calibration_only['Component Name'].str.contains('IPS')]\n",
    "quantification_blank_only_ips = quantification_blank[quantification_blank['Component Name'].str.contains('IPS')]\n",
    "\n",
    "# evaluate mean per component\n",
    "calibration_ips_area_averages = calibration_only_ips.groupby('Component Name')['Area'].mean()\n",
    "quantification_blank_ips_area_averages = quantification_blank_only_ips.groupby('Component Name')['Area'].mean()\n",
    "\n",
    "# evaluate IPS concentration in quantification\n",
    "ips_concentration = quantification_blank_only_ips['Actual Concentration'].mean() / extracted_sample_volume\n",
    "\n",
    "# create plot for comparison\n",
    "image_path = os.path.join(plot_directory, 'ips_areas.png')\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 8), sharey=True)\n",
    "calibration_only_ips.boxplot(column='Area', by='Component Name', ax=ax[0])\n",
    "ax[0].plot([np.nan] + calibration_ips_area_averages.to_list(), color='red', linestyle='', marker=\"o\", label='calibration average')\n",
    "quantification_blank_only_ips.boxplot(column='Area', by='Component Name', ax=ax[1])\n",
    "ax[1].plot([np.nan] + quantification_blank_ips_area_averages.to_list(), color='red', linestyle='', marker=\"o\", label='quantification average')\n",
    "fig.suptitle('')\n",
    "ax[0].set_title('Calibration: 4 ng/ml')\n",
    "ax[1].set_title(f'Quantification: {ips_concentration * extracted_sample_volume} ng per {extracted_sample_volume} ml (?)')\n",
    "ax[0].set_xticks(\n",
    "    ticks=range(len(calibration_ips_area_averages) + 1),\n",
    "    labels=[''] + calibration_ips_area_averages.index.to_list(), rotation=90\n",
    "    )\n",
    "ax[1].set_xticks(\n",
    "    ticks=range(len(quantification_blank_ips_area_averages) + 1),\n",
    "    labels=[''] + quantification_blank_ips_area_averages.index.to_list(), rotation=90\n",
    "    )\n",
    "ax[0].set_ylabel('IPS area')\n",
    "[this_ax.set_xlabel('') for this_ax in ax]\n",
    "plt.savefig(image_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# save IPS area comparison plot to excel file\n",
    "workbook = load_workbook(processed_filepath_xlsx)\n",
    "plot_sheet = workbook.create_sheet('Details IPS concentration')\n",
    "\n",
    "img = Image(os.path.join(plot_directory, 'ips_areas.png'))\n",
    "\n",
    "cell_position = plot_sheet.cell(row=1, column=1).coordinate\n",
    "plot_sheet.add_image(img, cell_position)\n",
    "\n",
    "workbook.save(processed_filepath_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block the data (calibration not included) is separated in the data junk used for mdl calculation,\n",
    "and the remaining data (where the PFAS are quantified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_for_mdl_evaluation = sample_input_data[sample_input_data['used for mdl calculation']].index\n",
    "\n",
    "if len(indices_for_mdl_evaluation) == 0:\n",
    "    mdl_only = None\n",
    "    quantification_only = quantification_blank\n",
    "    print(f'Be careful, no samples have been collected for the MDL calculation.')\n",
    "else:\n",
    "    mdl_only = quantification_blank[quantification_blank['Sample Index'].isin(indices_for_mdl_evaluation)]\n",
    "    quantification_only = quantification_blank[~quantification_blank['Sample Index'].isin(indices_for_mdl_evaluation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Detection Limits\n",
    "The following block computes method detection limits (MDL) based on average and standard deviation of selected samples (process blanks, etc.). \\\n",
    "The code uses instrument detection limits (IDL) for the PFAS compounds which could not detcted in the samples considerd for the evaluation of MDLs. \\\n",
    "For some comounds the IDL may not be included in the input files, then the default value is used.\\\n",
    "\n",
    "Moreover, a new column 'Below Detection Threshold' is introduces, which indicates all Calculated Concentration Values of PFAS quantification below the determined detection limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSgXlrjDRDkn"
   },
   "outputs": [],
   "source": [
    "# Make empty dataframe if blanks for MDL calculations are not available\n",
    "mdl = pd.DataFrame(index=pfas_components)\n",
    "mdl['Mean Concentration'] = [np.nan] * len(mdl)\n",
    "mdl['Std Concentration'] = [np.nan] * len(mdl)\n",
    "mdl['MDL'] = [np.nan] * len(mdl)\n",
    "mdl['IDL'] = [np.nan] * len(mdl)\n",
    "\n",
    "if not mdl_only is None:\n",
    "    blank_only_default = mdl_only[mdl_only['Component Name'].isin(pfas_components)]\n",
    "    # create data frame with average and standard deviation values for MDL calculation and caluclate MDL\n",
    "    mdl_mean = blank_only_default.groupby('Component Name')['Calculated Concentration'].mean()\n",
    "    mdl_std = blank_only_default.groupby('Component Name')['Calculated Concentration'].std()\n",
    "    for (index, value) in mdl_mean.items():\n",
    "        mdl.loc[index, 'Mean Concentration'] = value\n",
    "    for (index, value) in mdl_std.items():\n",
    "        mdl.loc[index, 'Std Concentration'] = value\n",
    "    mdl['MDL'] = mdl['Mean Concentration'] + 3 * mdl['Std Concentration']\n",
    "\n",
    "# Load idl values from idl input file\n",
    "idl = pd.read_csv(idl_filepath, index_col=0, low_memory=False, nrows=1)\n",
    "\n",
    "# Write each iql value in new column of mdl dataframe\n",
    "for row_index in mdl.index:\n",
    "    if row_index in idl.columns:\n",
    "        mdl.loc[row_index, 'IDL'] = idl[f'{row_index}'].to_list()[0]\n",
    "    else:\n",
    "        mdl.loc[row_index, 'IDL'] = idl_unknown\n",
    "        print(f'No IDL available for {row_index}, default value of {idl_unknown} is used.')\n",
    "\n",
    "mdl['Detection Threshold'] = mdl['MDL']\n",
    "mdl['Detection Threshold'].fillna(mdl.IDL, inplace=True)\n",
    "\n",
    "# write detection threshold to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    sample_input_data.to_excel(writer, sheet_name='Sample Input Data')\n",
    "    mdl.to_excel(writer, sheet_name='Detection Threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wSLW9sjGkPS"
   },
   "source": [
    "# Recovery Rates\n",
    "To avoid misunderstandings, in the following two abbreviations are extensively used:\n",
    "- IDA: isotope dilution analysis, also known as SS=surrogate standard or EIS=extracted internal standard\n",
    "- IPS: isotope performance standard, also known as IS=injection standard or NIS=non-extracted internal standard\n",
    "\n",
    "**The following two blocks calculate response factors from calibration data:**\\\n",
    "ratio of (i) calculated area of IDA * **actual concentration of IPS** and (ii) calculated area of IPS * actual concentration of IDA. \\\n",
    "The data is saved to an excel file and a boxplot of response factors is created.\n",
    "\n",
    "Note: The response factor calculation within sciex uses the ratio of (i) calculated area of IDA and (ii) calculated area of IPS * actual concentration of IDA. \\\n",
    "As the concentration of IPS is missing in the calculation, the response factors deviate by a factor of 4, which is the actual concentration of IPS in the calibration data. \\\n",
    "\n",
    "Note: The column 'Actual Concentratione' provides information on ng of internal standard added per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II-BctklGp7V"
   },
   "outputs": [],
   "source": [
    "# define funtion which calculates the IDA IPS ratio.\n",
    "# challenge - search right IPS row indicated in the Component Group Name of IDA.\n",
    "def calculate_ida_ips_ratio(data: pd.DataFrame, column_name:str, ) -> pd.DataFrame:\n",
    "    \"\"\"Calculates IDA area times IPS concentration divided by IPS area times IDA concentration and save the results in the indicated column.\n",
    "\n",
    "    :param data: Entire data junk (including all rows and the following columns:\n",
    "    Component Name, Sample Index, Component Group Name, Actual Concentration, Area, and IDA Average Response Factor\n",
    "    :type data: pd.DataFrame\n",
    "    :param column_name: name of column, the calculated ratio should be saved to\n",
    "    :type column_name: str\n",
    "    :return: Data junk only containing IDA rows with the corresponding ratio saved to new column\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # select only ida rows from input data\n",
    "    data_only_ida = data[data['Component Name'].str.contains('IDA')]\n",
    "    # initialize new column names\n",
    "    data_only_ida[[f'{column_name}', 'IPS Area', 'IPS Concentration']] = np.nan\n",
    "\n",
    "    # calculate recovery rate for every component, end every sample\n",
    "    for row_index in data_only_ida.index:\n",
    "        sample_index = data_only_ida.loc[row_index, 'Sample Index']\n",
    "        ips_channel_name = data_only_ida.loc[row_index, 'Component Group Name']\n",
    "\n",
    "        sample_name = data_only_ida.loc[row_index, 'Sample Name']\n",
    "        corresponding_ips_area_row = data[(\n",
    "            (data['Sample Index'] == sample_index) &\n",
    "            (data['Component Name'] == ips_channel_name) &\n",
    "            (data['Sample Name'] == sample_name)\n",
    "            )]\n",
    "        data_only_ida.loc[row_index, 'IPS Area'] = corresponding_ips_area_row['Area'].iloc[0]\n",
    "        data_only_ida.loc[row_index, 'IPS Concentration'] = corresponding_ips_area_row['Actual Concentration'].iloc[0]\n",
    "        data_only_ida.loc[row_index, f'{column_name}'] = \\\n",
    "            (data_only_ida.loc[row_index, 'Area'] * corresponding_ips_area_row['Actual Concentration'].iloc[0]) \\\n",
    "            / (corresponding_ips_area_row['Area'].iloc[0] * data_only_ida.loc[row_index, 'Actual Concentration'])\n",
    "    return data_only_ida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuWK_jF4GydT"
   },
   "outputs": [],
   "source": [
    "# Extract values of IDAs and IPS\n",
    "# Save basic sample information as well as areas of intensity peak, actual concentration and Component Group Name.\n",
    "# The 'Component Group Name' is useful to assoiciate the right IPS to each IDA.\n",
    "\n",
    "# calucluate IDA IPS ratio to compute response factors with calibration data\n",
    "calibration_only_ida = calculate_ida_ips_ratio(\n",
    "    data=calibration_only, column_name='Response Factor Mean',\n",
    "    )\n",
    "\n",
    "# create data frame with this response factor calculation (from scratch), the standard deviation and the original values evaluated by Sciex,\n",
    "response_factor = calibration_only_ida.groupby('Component Name', as_index=False)['Response Factor Mean'].mean()\n",
    "response_factor['Response Factor Std'] = calibration_only_ida.groupby('Component Name')['Response Factor Mean'].std().to_list()\n",
    "response_factor['Response Factor Sciex'] = calibration_only_ida.groupby('Component Name')['IDA Average Response Factor'].mean().to_list()\n",
    "response_factor.index = response_factor['Component Name']\n",
    "response_factor.drop(columns=['Component Name'], inplace=True)\n",
    "\n",
    "# write response factor to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    response_factor.to_excel(writer, sheet_name='Response Factor')\n",
    "\n",
    "# create and save response factor box plots\n",
    "image_path = os.path.join(plot_directory, 'response_factors.png')\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "calibration_only_ida.boxplot(column='Response Factor Mean', by='Component Name', ax=ax,)\n",
    "ax.plot([np.nan] + (response_factor['Response Factor Sciex'] * 4).to_list(), color='red', linestyle='', marker=\"o\",)\n",
    "ax.plot([np.nan] + (response_factor['Response Factor Sciex']).to_list(), color='blue', linestyle='', marker=\"o\",)\n",
    "fig.suptitle('')\n",
    "ax.set_title('')\n",
    "plt.ylabel('Response Factor (IDA area/IPS area)')\n",
    "plt.xticks(rotation=90)\n",
    "box_patch = mpatches.Patch(color='blue', fill=False, label='data')\n",
    "blue_patch = mpatches.Patch(color='blue', label='RF Sciex')\n",
    "red_patch = mpatches.Patch(color='red', label='4 * RF Sciex')\n",
    "plt.legend(handles=[box_patch, red_patch, blue_patch])\n",
    "plt.savefig(image_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# save response factor box plot to excel file\n",
    "workbook = load_workbook(processed_filepath_xlsx)\n",
    "plot_sheet = workbook.create_sheet('Details RF')\n",
    "\n",
    "img = Image(os.path.join(plot_directory, 'response_factors.png'))\n",
    "\n",
    "cell_position = plot_sheet.cell(row=1, column=1).coordinate\n",
    "plot_sheet.add_image(img, cell_position)\n",
    "\n",
    "workbook.save(processed_filepath_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYEDfn9hHBJP"
   },
   "source": [
    "The following block calculates recovery rates for each IDA compound in each sample.\n",
    "$$\n",
    "recovery~rate = \\frac{\\frac{area_{IDA~sample}~\\cdot~concentration_{IPS~sample}}{area_{IPS~sample}~\\cdot~concentration_{IDA~sample}}}{average(\\frac{area_{IDA~calibration}~\\cdot~concentration_{IPS~calibration}}{area_{IPS~calibration}~\\cdot~concentration_{IDA~calibration}})} = \\frac{ratio}{response~factor}\n",
    "$$\n",
    "\n",
    "Recovery rate computed within the SCIEX software is not normalized by concentration. So the recovery rate calculated by SCIEX reads:\n",
    "$$\n",
    "recovery~rate = \\frac{\\frac{area_{IDA~sample}}{area_{IPS~sample}}}{average(\\frac{area_{IDA~calibration}}{area_{IPS~calibration}})}\n",
    "$$\n",
    "\n",
    "The uncertainty of the recovery rate is indicated by a maximum error method - for now, you can just ignore this part...\n",
    "$$\n",
    "\\Delta recovery~rate = \\Delta response~factor \\cdot \\frac{ratio}{response~factor^{2}} + \\Delta ratio \\cdot \\frac{1}{response~factor}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to color excel according to threshold values / mask\n",
    "def highlight(data, flag):\n",
    "    '''Sets all data elements to red background, when flag is True. '''\n",
    "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
    "        return ['background-color: red' if v else '' for v in flag]\n",
    "    else:  # from .apply(axis=None)\n",
    "        return pd.DataFrame(np.where(flag, 'background-color: red', ''),\n",
    "                            index=data.index, columns=data.columns)\n",
    "    \n",
    "# function to determine if recovery rates are within given limit\n",
    "def flag_poor_recovery(data: pd.DataFrame, checked_column_name: str, new_column_name: str) -> tuple[pd.DataFrame, None]:\n",
    "    \"\"\"Checks if recovery rates are in defined threshold limits and appends check results as boolean column to data.\n",
    "    Additionally formats table for recovery rate.\n",
    "\n",
    "    :param data: Entire data junk including all rows and the following columns:\n",
    "    'Component Name', 'Sample Index', f'{checked_column_name}'\n",
    "    :type data: pd.DataFrame\n",
    "    :param checked_column_name: name of column which is used to check if values are in the given limits.\n",
    "    :type checked_column_name: str\n",
    "    :param new_column_name: name of column, the check results should be saved to.\n",
    "    :type new_column_name: str\n",
    "    :return: Original data junk with the new column (ckeck results) appended + formated pivot table containing recovery rates.\n",
    "    :rtype: tuple[pd.DataFrame, pd.DataFrame.style.Styler]\n",
    "    \"\"\"\n",
    "    data[f'{new_column_name}'] = np.nan\n",
    "    for component_ida in data['Component Name'].unique():\n",
    "        lower_threshold_recovery = recovery_thresholds_dict['lower threshold for recoveries [%]'][component_ida]\n",
    "        upper_threshold_recovery = recovery_thresholds_dict['upper threshold for recoveries [%]'][component_ida]\n",
    "        data.loc[data['Component Name'] == component_ida, f'{new_column_name}'] = True\n",
    "        data.loc[\n",
    "            (data['Component Name'] == component_ida) &\n",
    "            (data[f'{checked_column_name}'] > lower_threshold_recovery) &\n",
    "            (data[f'{checked_column_name}'] < upper_threshold_recovery)\n",
    "        , f'{new_column_name}'] = False\n",
    "\n",
    "    recovery_pivot = data.pivot_table(\n",
    "        index=('Sample Index',), columns='Component Name', values=f'{checked_column_name}', aggfunc='mean', dropna=False,\n",
    "    )\n",
    "\n",
    "    poor_recovery_pivot = data.pivot_table(\n",
    "        index=('Sample Index',), columns='Component Name', values=f'{new_column_name}', aggfunc='first', dropna=False,\n",
    "        )\n",
    "\n",
    "    # color cells based on recovery values\n",
    "    recovery_styled = recovery_pivot.style.apply(highlight, flag=poor_recovery_pivot, axis=None)\n",
    "\n",
    "    return data, recovery_styled\n",
    "\n",
    "def reindex_in_excel(filename: str, sheetname: str) -> None:\n",
    "    \"\"\"Function to rename index columns in excel.\"\"\"\n",
    "\n",
    "    # load excel file\n",
    "    workbook = load_workbook(filename=filename)\n",
    "    # open workbook\n",
    "    sheet = workbook.active\n",
    "    # get right sheet\n",
    "    sheet = workbook[sheetname]\n",
    "    # read in index column\n",
    "    sample_index = [column.value for column in sheet['A']]\n",
    "  \n",
    "    # change index column values\n",
    "    if sample_index[0] == 'Sample Index':\n",
    "        for row_index in range(1, len(sample_index)):\n",
    "            sheet.cell(row=row_index + 1, column=1).value = index_mapper[sample_index[row_index]]\n",
    "    \n",
    "    #save the file\n",
    "    workbook.save(filename=filename)\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aznnAlxKHJ6p"
   },
   "outputs": [],
   "source": [
    "# Select ida rows from quantification data and calculate ida ips ratio\n",
    "# function is defined in previous block\n",
    "quantification_ida = calculate_ida_ips_ratio(\n",
    "    data=quantification_blank, column_name=\"IDA-IPS Ratio\",\n",
    "    )\n",
    "\n",
    "recovery_table = pd.DataFrame(columns=[\n",
    "    'Sample Index', 'Component Name', 'IDA-IPS Ratio', 'IDA-IPS Ratio Std',\n",
    "    'Response Factor Mean', 'Response Factor Std', 'Recovery Rate', 'Recovery Rate Uncertainty',\n",
    "    'Recovery Rate Sciex'\n",
    "    ])\n",
    "\n",
    "# delete IDA and IPS values from one method if 'core' or 'extended' is selected,\n",
    "# and use the average of both methods in the alternative case.\n",
    "index = 0\n",
    "\n",
    "for sample_index in quantification_ida['Sample Index'].unique():\n",
    "    for component_ida in quantification_ida['Component Name'].unique():\n",
    "        selected_sample_ida = quantification_ida.loc[(\n",
    "            (quantification_ida['Sample Index'] == sample_index) & \n",
    "            (quantification_ida['Component Name'] == component_ida)\n",
    "        ), :]\n",
    "        response_factor_mean = response_factor.loc[component_ida, 'Response Factor Mean']\n",
    "        response_factor_std = response_factor.loc[component_ida, 'Response Factor Std']\n",
    "        if len(selected_sample_ida.index) == 0:\n",
    "            print(f'Component {component_ida} is not available for sample {sample_index}')\n",
    "            ida_ips_ratio_mean = np.nan\n",
    "            ida_ips_ratio_std = np.nan\n",
    "            recovery_rate_sciex = np.nan\n",
    "        elif len(selected_sample_ida.index) == 1:\n",
    "            ida_ips_ratio_mean = selected_sample_ida['IDA-IPS Ratio'].values[0]\n",
    "            recovery_rate_sciex = selected_sample_ida['Reported Recovery'].values[0]\n",
    "            ida_ips_ratio_std = 0.1 * ida_ips_ratio_mean\n",
    "        else:\n",
    "            if recovery_data == 'average':\n",
    "                ida_ips_ratio_mean = selected_sample_ida['IDA-IPS Ratio'].mean()\n",
    "                ida_ips_ratio_std = selected_sample_ida['IDA-IPS Ratio'].std()\n",
    "                recovery_rate_sciex = selected_sample_ida['Reported Recovery'].mean()\n",
    "            elif recovery_data == 'core':\n",
    "                ida_ips_ratio_mean = selected_sample_ida.loc[\n",
    "                    selected_sample_ida['Sample Name'].str.contains('Core'), 'IDA-IPS Ratio'\n",
    "                    ].values[0]\n",
    "                ida_ips_ratio_std = 0.1 * ida_ips_ratio_mean\n",
    "                recovery_rate_sciex = selected_sample_ida.loc[\n",
    "                    selected_sample_ida['Sample Name'].str.contains('Core'), 'Reported Recovery'\n",
    "                    ].values[0]\n",
    "            elif recovery_data == 'extended':\n",
    "                ida_ips_ratio_mean = selected_sample_ida.loc[\n",
    "                    selected_sample_ida['Sample Name'].str.contains('Ext'), 'IDA-IPS Ratio'\n",
    "                    ].values[0]\n",
    "                ida_ips_ratio_std = 0.1 * ida_ips_ratio_mean\n",
    "                recovery_rate_sciex = selected_sample_ida.loc[\n",
    "                    selected_sample_ida['Sample Name'].str.contains('Ext'), 'Reported Recovery'\n",
    "                    ].values[0]\n",
    "            else:\n",
    "                raise Exception(\"\"\"\n",
    "                Invalid input for variable recovery_data, use either 'core', 'extended', or 'average'.\n",
    "                            \"\"\")\n",
    "\n",
    "        recovery_rate = 100 * ida_ips_ratio_mean / response_factor_mean\n",
    "        recovery_rate_uncertainty = 100 * (\n",
    "            response_factor_std * ida_ips_ratio_mean / response_factor_mean ** 2 + \\\n",
    "                ida_ips_ratio_std / response_factor_mean\n",
    "                )\n",
    "        \n",
    "        recovery_table.loc[index] = [\n",
    "            sample_index, component_ida, ida_ips_ratio_mean, ida_ips_ratio_std,\n",
    "            response_factor_mean, response_factor_std, recovery_rate, recovery_rate_uncertainty,\n",
    "            recovery_rate_sciex\n",
    "            ]\n",
    "        index +=1\n",
    "\n",
    "# check if recovery rate is within indicated limit for each ida and save check results to 'Poor Recovery column\n",
    "recovery_table, recovery_styled = flag_poor_recovery(data=recovery_table, checked_column_name='Recovery Rate', new_column_name='Poor Recovery')\n",
    "recovery_table, recovery_sciex_styled = flag_poor_recovery(data=recovery_table, checked_column_name='Recovery Rate Sciex', new_column_name='Poor Recovery Sciex')\n",
    "\n",
    "# Write recovery rate to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    recovery_thresholds.to_excel(writer, sheet_name='Recovery Thresholds')\n",
    "    recovery_styled.to_excel(writer, sheet_name='Recovery Rate')\n",
    "    recovery_sciex_styled.to_excel(writer, sheet_name='Sciex Recovery Rate')\n",
    "    \n",
    "reindex_in_excel(filename=processed_filepath_xlsx, sheetname='Recovery Rate')\n",
    "reindex_in_excel(filename=processed_filepath_xlsx, sheetname='Sciex Recovery Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells generate plots of recovery rates over all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for recovery rates\n",
    "image_path = os.path.join(plot_directory, 'recovery_rates_box.png')\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "recovery_table.boxplot(column='Recovery Rate', by='Component Name', ax=ax,)\n",
    "ax.set_ylim([0,500])\n",
    "fig.suptitle('')\n",
    "ax.set_title('')\n",
    "plt.ylabel('Recovery Rate')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.savefig(image_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot data as points for recovery rates:\n",
    "plot_data = recovery_table.groupby('Sample Index')  # group data for plotting\n",
    "cmap = plt.cm.get_cmap('tab20', len(plot_data)) # initialize colours\n",
    "image_path = os.path.join(plot_directory, 'recovery_rates.png')  # set path for figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for index, (title, group) in enumerate(plot_data):\n",
    "    group.set_index(group['Component Name'], inplace=True)\n",
    "    group.sort_index(inplace=True)\n",
    "    group.drop_duplicates(keep='first', inplace=True)\n",
    "    group.plot(\n",
    "        y='Recovery Rate', ax=ax, marker='.', linestyle='None', label=index_mapper[title],\n",
    "        grid=True, color = cmap(index),\n",
    "    )\n",
    "ax.set_ylim([0,500])\n",
    "fig.suptitle('')\n",
    "ax.set_xticks(range(len(group)))\n",
    "ax.set_xticklabels(group['Component Name'], rotation=90)\n",
    "ax.set_title('')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Recovery Rate')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5))\n",
    "plt.savefig(image_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# save recovery rate plot to excel file\n",
    "workbook = load_workbook(processed_filepath_xlsx)\n",
    "plot_sheet = workbook.create_sheet('Details Recovery Rates')\n",
    "\n",
    "img1 = Image(os.path.join(plot_directory, 'recovery_rates_box.png'))\n",
    "img1.anchor = 'A1'\n",
    "plot_sheet.column_dimensions['A'].width = img1.width / 6\n",
    "plot_sheet.row_dimensions[1].height = img.height\n",
    "plot_sheet.add_image(img1)\n",
    "\n",
    "img2 = Image(os.path.join(plot_directory, 'recovery_rates.png'))\n",
    "img2.anchor = 'B1'\n",
    "plot_sheet.column_dimensions['B'].width = img2.width / 6\n",
    "plot_sheet.add_image(img2)\n",
    "\n",
    "workbook.save(processed_filepath_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YKdBVv8RJae"
   },
   "source": [
    "# Mass Channel Ratios\n",
    "The following block evaluates ratio of calculated concentration from default channel and _TOF MS channel for all PFAS components and standards and all samples.\n",
    "\n",
    "Moreover a new column 'Channel Ratio . x' is intrudoced, which indicates all calculated concentration values which deviate more than x % between channels.\n",
    "\n",
    "x is the variable 'allowed_channel_deviation' you set in the third code block on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5i5K42m8Ra-9"
   },
   "outputs": [],
   "source": [
    "# Assign TOF channel to each PFAS and save calculated concentration to new column\n",
    "\n",
    "# select columns\n",
    "selected_columns = [\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time', 'Component Name', 'Calculated Concentration',\n",
    "    ]\n",
    "mass_channel_data = quantification_blank[selected_columns]\n",
    "\n",
    "# select only channel names of fragmented masses for descriptive data of channel ratios\n",
    "mass_channel_data = quantification_blank[quantification_blank['Component Name'].isin(components_fragmented)]\n",
    "# initialize calculated concentration TOF column in descriptive data of channel ratios\n",
    "mass_channel_data['Calculated Concentration TOF'] = np.nan\n",
    "for row_index in mass_channel_data.index:\n",
    "    # get sample index and name of corresponding tof channel\n",
    "    sample_index = mass_channel_data.loc[row_index, 'Sample Index']\n",
    "    tof_channel_index = components_fragmented.index(mass_channel_data.loc[row_index, 'Component Name'])\n",
    "    tof_channel_name = components_precursor[tof_channel_index]\n",
    "    # select data of corresponding tof channel\n",
    "    tof_channel_row = quantification_blank[(\n",
    "        (quantification_blank['Component Name'] == tof_channel_name) &\n",
    "        (quantification_blank['Sample Index'] == sample_index)\n",
    "    )]\n",
    "    # right tof channel concentration to new data frame\n",
    "    mass_channel_data.loc[row_index, 'Calculated Concentration TOF'] = \\\n",
    "        tof_channel_row.loc[:,'Calculated Concentration'].to_list()[0]\n",
    "\n",
    "# Calculate percentage deviation of the channels\n",
    "mass_channel_data['Channel Ratio'] = (\n",
    "    200 * (mass_channel_data['Calculated Concentration'] - mass_channel_data['Calculated Concentration TOF']) \\\n",
    "        / (mass_channel_data['Calculated Concentration'] + mass_channel_data['Calculated Concentration TOF'])\n",
    ").round(decimals=1)\n",
    "\n",
    "# Introduce new column where everything below method detection limit is marked\n",
    "mass_channel_data[f'Channel Ratio > {allowed_channel_deviation}'] = abs(mass_channel_data['Channel Ratio']) > allowed_channel_deviation\n",
    "\n",
    "# Put channel ratio in pivot table\n",
    "channel_ratio = mass_channel_data.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values='Channel Ratio', aggfunc='mean', dropna=False,\n",
    ")[pfas_components]\n",
    "channel_ratio_flag = mass_channel_data.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values=f'Channel Ratio > {allowed_channel_deviation}', aggfunc='first', dropna=False,\n",
    ")[pfas_components]\n",
    "\n",
    "# color cells based on threshold values\n",
    "channel_ratio_styled = channel_ratio.style.apply(highlight, flag=channel_ratio_flag, axis=None).relabel_index(\n",
    "    labels=[index_mapper[sample_index] for sample_index in channel_ratio.index], axis=0, level=0,\n",
    ")\n",
    "\n",
    "# Write to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    channel_ratio_styled.to_excel(writer, sheet_name='Channel Ratio')\n",
    "reindex_in_excel(filename=processed_filepath_xlsx, sheetname='Channel Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block evaluates ratio of areas from default channel and _TOF MS channel for all standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "selected_columns = [\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time', 'Component Name', 'Area',\n",
    "    ]\n",
    "quantification_standards = quantification_blank[selected_columns]\n",
    "\n",
    "# Get only default channels of standards\n",
    "quantification_standards_default = quantification_standards[quantification_standards['Component Name'].isin(ida_ips_fragmented)]\n",
    "\n",
    "# initialize respective TOF concentration channel and get respective values\n",
    "quantification_standards_default['Area TOF'] = np.nan\n",
    "for row_index in quantification_standards_default.index:\n",
    "    sample_name = quantification_standards_default.loc[row_index, 'Sample Name']\n",
    "    tof_channel_index = ida_ips_fragmented.index(quantification_standards_default.loc[row_index, 'Component Name'])\n",
    "    tof_channel_name = ida_ips_precursor[tof_channel_index]\n",
    "    tof_channel_row = quantification_standards[(\n",
    "        (quantification_standards['Component Name'] == tof_channel_name) &\n",
    "        (quantification_standards['Sample Name'] == sample_name)\n",
    "    )]\n",
    "    if tof_channel_row.empty:\n",
    "        quantification_standards_default.loc[row_index, 'Area TOF'] = np.nan\n",
    "    else:\n",
    "        quantification_standards_default.loc[row_index, 'Area TOF'] = \\\n",
    "            tof_channel_row.loc[:,'Area'].values[0]\n",
    "\n",
    "# Calculate percentage deviation of the channels\n",
    "quantification_standards_default['Channel Ratio'] = (\n",
    "    200 * (quantification_standards_default['Area'] - quantification_standards_default['Area TOF']) \\\n",
    "        / (quantification_standards_default['Area'] + quantification_standards_default['Area TOF'])\n",
    ").round(decimals=1)\n",
    "\n",
    "# Introduce new column where everything below method detection limit is marked\n",
    "quantification_standards_default[f'Channel Ratio > {allowed_channel_deviation}'] = abs(quantification_standards_default['Channel Ratio']) > allowed_channel_deviation\n",
    "\n",
    "# Put channel ratio in pivot table\n",
    "channel_ratio = quantification_standards_default.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values='Channel Ratio', aggfunc='first', dropna=False,\n",
    ")[ida_ips_fragmented]\n",
    "channel_ratio_flag = quantification_standards_default.pivot_table(\n",
    "    index=('Sample Name',), columns='Component Name', values=f'Channel Ratio > {allowed_channel_deviation}', aggfunc='first', dropna=False,\n",
    ")[ida_ips_fragmented]\n",
    "\n",
    "# color cells based on threshold values\n",
    "channel_ratio_styled = channel_ratio.style.apply(highlight, flag=channel_ratio_flag, axis=None)\n",
    "\n",
    "# Write to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    channel_ratio_styled.to_excel(writer, sheet_name='Standards Channel Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouputs and data merging\n",
    "\n",
    "In the following code block all relevant data and parameters are combined in one common table quantification_pfas_default. The table has one row for each PFAS component in each sample: \\\n",
    "(1) The right recovery rate is assigned to each PFAS. Each PFAS component has a corresponding IDA - the recovery rate is deduced from the recovery rate of the corresponding IDA. The assignment works for each sample by using the 'IS Name' column which provides information on which IDA standard is associated to which PFAS.\\\n",
    "(2) detection thresholds are appended to the table as well as a boolean column indicating if the detected values are below the threshold. \\\n",
    "(3) basic information about the sample are also extracted from the input table and included \\\n",
    "(4) mass channel ratios are assigned to each PFAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data frame for following assignments, select only needed columns and use reasonable naming.\n",
    "selected_columns = [\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time', 'Component Name', 'Calculated Concentration',\n",
    "    'Area', 'IS Name',\n",
    "    ]\n",
    "quantification_pfas_default = quantification_only[selected_columns]\n",
    "quantification_pfas_default.rename(columns={'IS Name': 'IDA Name'}, inplace=True)\n",
    "\n",
    "# Get only PFAS default channels\n",
    "quantification_pfas_default = quantification_pfas_default[\n",
    "    quantification_pfas_default['Component Name'].isin(pfas_components)\n",
    "    ]\n",
    "\n",
    "# Assign right recovery rate and uncertainty to each PFAS compound\n",
    "# initialize new columns\n",
    "quantification_pfas_default[[\n",
    "    'IDA Area', 'IDA Concentration', 'IPS Name', 'IPS Area', 'IPS Concentration',\n",
    "    'Recovery Rate', 'Recovery Rate Uncertainty', 'Poor Recovery', 'Recovery Rate Sciex', \n",
    "    'Detection Threshold', 'Sample Quantity', 'Sample Unit',\n",
    "    'Channel Ratio', 'Channel Ratio > {allowed_channel_deviation}',\n",
    "]] = np.nan\n",
    "quantification_pfas_default['Below Detection Threshold'] = False\n",
    "\n",
    "for row_index in quantification_pfas_default.index:\n",
    "    sample_index = quantification_pfas_default.loc[row_index, 'Sample Index']\n",
    "    sample_name = quantification_pfas_default.loc[row_index, 'Sample Name']\n",
    "    ida_channel_name = quantification_pfas_default.loc[row_index, 'IDA Name']\n",
    "    component_name = quantification_pfas_default.loc[row_index, 'Component Name']\n",
    "\n",
    "    recovery_row = recovery_table[(\n",
    "        (recovery_table['Component Name'] == ida_channel_name) &\n",
    "        (recovery_table['Sample Index'] == sample_index)\n",
    "    )]\n",
    "\n",
    "    ida_row = quantification_ida[(\n",
    "            (quantification_ida['Component Name'] == ida_channel_name) &\n",
    "            (quantification_ida['Sample Index'] == sample_index) &\n",
    "            (quantification_ida['Sample Name'] == sample_name)\n",
    "    )]\n",
    "\n",
    "    channel_row = mass_channel_data[(\n",
    "            (mass_channel_data['Component Name'] == component_name) &\n",
    "            (mass_channel_data['Sample Index'] == sample_index)\n",
    "    )]\n",
    "\n",
    "    detection_threshold = mdl.loc[component_name, 'Detection Threshold']\n",
    "    \n",
    "    if ida_channel_name in quantification_ida['Component Name'].to_list():\n",
    "        quantification_pfas_default.loc[row_index,'IDA Area'] = ida_row.loc[:,'Area'].values[0]\n",
    "        quantification_pfas_default.loc[row_index,'IDA Concentration'] = ida_row.loc[:,'Actual Concentration'].values[0]\n",
    "        quantification_pfas_default.loc[row_index,'IPS Area'] = ida_row.loc[:,'IPS Area'].values[0]\n",
    "        quantification_pfas_default.loc[row_index,'IPS Concentration'] = ida_row.loc[:,'IPS Concentration'].values[0]\n",
    "        quantification_pfas_default.loc[row_index, 'IPS Name'] = \\\n",
    "            ida_row.loc[:,'Component Group Name'].values[0]\n",
    "        quantification_pfas_default.loc[row_index, 'Recovery Rate'] = \\\n",
    "            recovery_row.loc[:,'Recovery Rate'].values[0]\n",
    "        quantification_pfas_default.loc[row_index, 'Recovery Rate Uncertainty'] = \\\n",
    "            recovery_row.loc[:,'Recovery Rate Uncertainty'].values[0]\n",
    "        quantification_pfas_default.loc[row_index, 'Poor Recovery'] = \\\n",
    "            recovery_row.loc[:,'Poor Recovery'].values[0]\n",
    "        quantification_pfas_default.loc[row_index, 'Recovery Rate Sciex'] = \\\n",
    "            recovery_row.loc[:,'Recovery Rate Sciex'].values[0]\n",
    "\n",
    "    quantification_pfas_default.loc[row_index, 'Detection Threshold'] = detection_threshold\n",
    "    if not np.isnan(quantification_pfas_default.loc[row_index, 'Calculated Concentration']):\n",
    "        if quantification_pfas_default.loc[row_index, 'Calculated Concentration'] < detection_threshold:\n",
    "            quantification_pfas_default.loc[row_index, 'Below Detection Threshold'] = True\n",
    "    quantification_pfas_default.loc[row_index, 'Sample Quantity'] = \\\n",
    "        sample_input_data.loc[sample_index, 'volume/weight']\n",
    "    quantification_pfas_default.loc[row_index, 'Sample Unit'] = \\\n",
    "        sample_input_data.loc[sample_index, 'unit [ml or g]']\n",
    "\n",
    "    quantification_pfas_default.loc[row_index, 'Channel Ratio'] = \\\n",
    "        channel_row.loc[:, 'Channel Ratio'].values[0]\n",
    "    quantification_pfas_default.loc[row_index, f'Channel Ratio > {allowed_channel_deviation}'] = \\\n",
    "        channel_row.loc[:, f'Channel Ratio > {allowed_channel_deviation}'].values[0]\n",
    "\n",
    "for column in ['Recovery Rate', 'Recovery Rate Uncertainty', 'Recovery Rate Sciex']:\n",
    "    quantification_pfas_default[f'{column}'] = quantification_pfas_default[f'{column}'].round(decimals=1)\n",
    "\n",
    "# Put computed recovery rate in pivot table\n",
    "recovery_extended_pivot = quantification_pfas_default.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values='Recovery Rate', aggfunc='mean', dropna=False,\n",
    ")[pfas_components]\n",
    "poor_recovery_extended_pivot = quantification_pfas_default.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values=f'Poor Recovery', aggfunc='first', dropna=False,\n",
    ")[pfas_components]\n",
    "\n",
    "# color cells based on recovery values\n",
    "recovery_extended_styled = recovery_extended_pivot.style.apply(\n",
    "    highlight, flag=poor_recovery_extended_pivot, axis=None\n",
    "    ).relabel_index(\n",
    "        labels=[index_mapper[sample_index] for sample_index in recovery_extended_pivot.index],\n",
    "        axis=0, level=0,\n",
    "    )\n",
    "\n",
    "# Write to excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    recovery_extended_styled.to_excel(writer, sheet_name='Recovery Rate Extended')\n",
    "reindex_in_excel(filename=processed_filepath_xlsx, sheetname='Recovery Rate Extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs\n",
    "The following code block writes final concentration table with all information to excel. It uses concentration values and indicates all values below detection threshold with '< MDL', all channel deviation above x % with '> CR' and all recovery rates below or above the indicated threshold values with 'Poor Recovery'.\n",
    "In addition, concentration values are converted to ng\\g or ng\\l respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFWdAgN-6HGJ"
   },
   "outputs": [],
   "source": [
    "def flag_values(data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Flags column values (most probably concentrations) with channel ratio, recovery rates and MDLs.\n",
    "\n",
    "    :param data: data frame containing columns 'Channel Ratio', 'Poor Recovery', and 'Below Detection Threshold',\n",
    "    as well as the column you indicated.\n",
    "    :type data: pd.DataFrame\n",
    "    :param column: Colun name of data frame to be filtered or flagged.\n",
    "    :type column: str\n",
    "    :return: Data frame, where the column data is flagged.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    final_table = data[['Sample Name', 'Sample Index', 'Component Name', column]]\n",
    "    final_table.loc[final_table.index[quantification_pfas_default[f'Channel Ratio > {allowed_channel_deviation}']], column] = '> CR'\n",
    "    final_table.loc[final_table.index[quantification_pfas_default['Poor Recovery']], column] = 'Poor Recovery'\n",
    "    final_table.loc[final_table.index[quantification_pfas_default['Below Detection Threshold']], column] = '< MDL'\n",
    "    return final_table\n",
    "\n",
    "# Transform concentration to ng/g or ng/l, depending on your sample_unit\n",
    "quantification_pfas_default[f'Concentration in ng per Unit'] = quantification_pfas_default['Calculated Concentration'] \\\n",
    "    / quantification_pfas_default['Sample Quantity']\n",
    "\n",
    "# Flag concentration values with channel ratio, recovery rates and detection threshold.\n",
    "calculated_concentration = flag_values(data=quantification_pfas_default, column='Calculated Concentration')\n",
    "calculated_concentration_II = flag_values(data=quantification_pfas_default, column=f'Concentration in ng per Unit')\n",
    "\n",
    "# Pivot concentration tables.\n",
    "calculated_concentration = calculated_concentration.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values='Calculated Concentration', aggfunc='first', dropna=False,\n",
    ")\n",
    "calculated_concentration.rename(index=index_mapper, inplace=True)\n",
    "calculated_concentration = calculated_concentration[pfas_components]\n",
    "\n",
    "calculated_concentration_II = calculated_concentration_II.pivot_table(\n",
    "    index=('Sample Index',), columns='Component Name', values=f'Concentration in ng per Unit', aggfunc='first', dropna=False,\n",
    ")\n",
    "calculated_concentration_II.rename(index=index_mapper, inplace=True)\n",
    "calculated_concentration_II = calculated_concentration_II[pfas_components]\n",
    "\n",
    "# Write pivot tables to existing excel file\n",
    "with pd.ExcelWriter(processed_filepath_xlsx, engine='openpyxl', mode='a') as writer:\n",
    "    calculated_concentration.to_excel(writer, sheet_name='Concentration Table')\n",
    "    calculated_concentration_II.to_excel(writer, sheet_name=f'Concentration (ng per Unit)')\n",
    "\n",
    "# Write long format data to csv\n",
    "quantification_pfas_default = quantification_pfas_default[[\n",
    "    'Sample Name', 'Sample Index', 'Acquisition Date & Time','Component Name',\n",
    "    'Area', 'Calculated Concentration', 'Concentration in ng per Unit', 'Sample Quantity', 'Sample Unit',\n",
    "    'IDA Name', 'IDA Area', 'IDA Concentration', 'IPS Name', 'IPS Area', 'IPS Concentration',\n",
    "    'Recovery Rate', 'Recovery Rate Uncertainty', 'Recovery Rate Sciex', 'Poor Recovery',\n",
    "    'Detection Threshold', 'Below Detection Threshold',\n",
    "    'Channel Ratio', f'Channel Ratio > {allowed_channel_deviation}', \n",
    "]]\n",
    "quantification_pfas_default.sort_values(by=['Sample Index', 'Component Name'], inplace=True)\n",
    "\n",
    "quantification_pfas_default.to_csv(processed_filepath_csv)\n",
    "\n",
    "# Print output\n",
    "display(calculated_concentration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lohmann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
